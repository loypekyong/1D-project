{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group THE BOYS 1D ML project\n",
    "1. Loy Pek Yong 1004475\n",
    "2. Lim Yongqing 1005955\n",
    "3. Sim Reynard Simano 1006307\n",
    "\n",
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries + reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "\n",
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    r1 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r1]\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "\n",
    "with open('./ES/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r3 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r3]\n",
    "\n",
    "\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    r4 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r4]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r5 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r5]\n",
    "\n",
    "with open('./RU/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r6 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_train = pd.DataFrame(data = ES_train_tup, columns = [\"word\", \"label\"])\n",
    "ES_dev_in = pd.DataFrame(data = ES_dev_in_tup, columns = [\"word\"])\n",
    "ES_dev_out = pd.DataFrame(data = ES_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "RU_train = pd.DataFrame(data = RU_train_tup, columns = [\"word\", \"label\"])\n",
    "RU_dev_in = pd.DataFrame(data = RU_dev_in_tup, columns = [\"word\"])\n",
    "RU_dev_out = pd.DataFrame(data = RU_dev_out_tup, columns = [\"word\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimate_emission_parameters(train):\n",
    "    # word_column = train[\"word\"]\n",
    "    # label_column = train[\"label\"]\n",
    "\n",
    "    label_count = {}\n",
    "    emission_count = {}\n",
    "\n",
    "    for row in range(train.shape[0]):\n",
    "        word = train.iloc[row, 0]\n",
    "        label = train.iloc[row, 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "\n",
    "        if (word, label) not in emission_count:\n",
    "            emission_count[(word, label)] = 1\n",
    "        else:\n",
    "            emission_count[(word, label)] += 1\n",
    "\n",
    "    emission_parameter = {}\n",
    "    for word, label in emission_count:\n",
    "        emission_parameter[(word, label)] = emission_count[(word, label)] / label_count[label]\n",
    "\n",
    "    return emission_parameter, emission_count, label_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 0\n",
    "    special_word = '#UNK#'\n",
    "    test_copy = test_set.copy()\n",
    "    #train word list is all the first argument in keys of emission counts\n",
    "    train_word_list= [word for word, label in emission_counts.keys()]\n",
    "    for i in range(len(test_copy[\"word\"])):\n",
    "        word = test_copy[\"word\"][i]\n",
    "        if word not in train_word_list:\n",
    "            test_copy[\"word\"][i] = special_word\n",
    "            k+=1\n",
    "\n",
    "    for word in test_copy[\"word\"]:\n",
    "        for label in label_counts:\n",
    "            if word == special_word:\n",
    "                emission_parameter[(word, label)] = k / (label_counts[label] + k)\n",
    "            elif ( (word, label) in emission_parameter ):\n",
    "                emission_parameter[(word, label)] = emission_counts[(word, label)] / (label_counts[label] + k)\n",
    "                \n",
    "    # print(emission_parameter)        \n",
    "    return emission_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 of q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(training_set, test_set):\n",
    "    emission_parameter, emission_count, label_count = train_estimate_emission_parameters(training_set)\n",
    "    emission_parameter = upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_count, label_count)\n",
    "    test_set[\"label\"] = \"\"\n",
    "\n",
    "    #label equals to argmax of emission parameter\n",
    "    for row in range(len(test_set)):\n",
    "        word = test_set.iloc[row][\"word\"]\n",
    "        max_val = 0\n",
    "        max_label = \"\"\n",
    "        train_word_list = [word for word, label in emission_count.keys()]\n",
    "        \n",
    "        for label in label_count:\n",
    "            if (word, label) in emission_parameter:\n",
    "                if emission_parameter[(word, label)] > max_val:\n",
    "                    max_val = emission_parameter[(word, label)]\n",
    "                    max_label = label\n",
    "                    \n",
    "            else:\n",
    "                if word not in train_word_list:\n",
    "                    # get max label from word #UNK#\n",
    "                    if (\"#UNK#\", label) in emission_parameter:\n",
    "                        if emission_parameter[(\"#UNK#\", label)] > max_val:\n",
    "                            max_val = emission_parameter[(\"#UNK#\", label)]\n",
    "                            max_label = label\n",
    "        test_set.loc[row, \"label\"] = max_label\n",
    "                    \n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting + saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_dev_p1_out = sentiment_analysis(ES_train, ES_dev_in)\n",
    "ES_dev_p1_out_r = ES_dev_p1_out.replace('null', pd.NA)\n",
    "ES_dev_p1_out_r.to_csv(\"./ES/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "    \n",
    "RU_dev_p1_out = sentiment_analysis(RU_train, RU_dev_in)\n",
    "RU_dev_p1_out_r = RU_dev_p1_out.replace('null', pd.NA)\n",
    "RU_dev_p1_out_r.to_csv(\"./RU/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1144\n",
      "\n",
      "#Correct Entity : 181\n",
      "Entity  precision: 0.1582\n",
      "Entity  recall: 0.7904\n",
      "Entity  F: 0.2637\n",
      "\n",
      "#Correct Sentiment : 120\n",
      "Sentiment  precision: 0.1049\n",
      "Sentiment  recall: 0.5240\n",
      "Sentiment  F: 0.1748\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1467\n",
      "\n",
      "#Correct Entity : 288\n",
      "Entity  precision: 0.1963\n",
      "Entity  recall: 0.7404\n",
      "Entity  F: 0.3103\n",
      "\n",
      "#Correct Sentiment : 174\n",
      "Sentiment  precision: 0.1186\n",
      "Sentiment  recall: 0.4473\n",
      "Sentiment  F: 0.1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result.stdout.decode())\n",
    "\n",
    "command = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "EStrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "EStrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in EStrain ]\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUtrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUtrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in RUtrain ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reference transition dictionary (all transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_dict = {\n",
    "    (\"O\", \"O\"): 0,\n",
    "    (\"O\", \"B-positive\"):0,\n",
    "    (\"O\", \"B-negative\"):0,\n",
    "    (\"O\", \"B-neutral\"):0,\n",
    "    (\"O\", \"I-positive\"):0,\n",
    "    (\"O\", \"I-negative\"):0,\n",
    "    (\"O\", \"I-neutral\"):0,\n",
    "    (\"O\", \"START\"):0,\n",
    "    (\"O\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-positive\", \"O\"):0,\n",
    "    (\"B-positive\", \"B-positive\"):0,\n",
    "    (\"B-positive\", \"B-negative\"):0,\n",
    "    (\"B-positive\", \"B-neutral\"):0,\n",
    "    (\"B-positive\", \"I-positive\"):0,\n",
    "    (\"B-positive\", \"I-negative\"):0,\n",
    "    (\"B-positive\", \"I-neutral\"):0,\n",
    "    (\"B-positive\", \"START\"):0,\n",
    "    (\"B-positive\", \"STOP\"):0,\n",
    "    \n",
    "    \n",
    "    (\"B-negative\", \"O\"):0,\n",
    "    (\"B-negative\", \"B-positive\"):0,\n",
    "    (\"B-negative\", \"B-negative\"):0,\n",
    "    (\"B-negative\", \"B-neutral\"):0,\n",
    "    (\"B-negative\", \"I-positive\"):0,\n",
    "    (\"B-negative\", \"I-negative\"):0,\n",
    "    (\"B-negative\", \"I-neutral\"):0,\n",
    "    (\"B-negative\", \"START\"):0,\n",
    "    (\"B-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-neutral\", \"O\"):0,\n",
    "    (\"B-neutral\", \"B-positive\"):0,\n",
    "    (\"B-neutral\", \"B-negative\"):0,\n",
    "    (\"B-neutral\", \"B-neutral\"):0,\n",
    "    (\"B-neutral\", \"I-positive\"):0,\n",
    "    (\"B-neutral\", \"I-negative\"):0,\n",
    "    (\"B-neutral\", \"I-neutral\"):0,\n",
    "    (\"B-neutral\", \"START\"):0,\n",
    "    (\"B-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-positive\", \"O\"):0,\n",
    "    (\"I-positive\", \"B-positive\"):0,\n",
    "    (\"I-positive\", \"B-negative\"):0,\n",
    "    (\"I-positive\", \"B-neutral\"):0,\n",
    "    (\"I-positive\", \"I-positive\"):0,\n",
    "    (\"I-positive\", \"I-negative\"):0,\n",
    "    (\"I-positive\", \"I-neutral\"):0,\n",
    "    (\"I-positive\", \"START\"):0,\n",
    "    (\"I-positive\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-negative\", \"O\"):0,\n",
    "    (\"I-negative\", \"B-positive\"):0,\n",
    "    (\"I-negative\", \"B-negative\"):0,\n",
    "    (\"I-negative\", \"B-neutral\"):0,\n",
    "    (\"I-negative\", \"I-positive\"):0,\n",
    "    (\"I-negative\", \"I-negative\"):0,\n",
    "    (\"I-negative\", \"I-neutral\"):0,\n",
    "    (\"I-negative\", \"START\"):0,\n",
    "    (\"I-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-neutral\", \"O\"):0,\n",
    "    (\"I-neutral\", \"B-positive\"):0,\n",
    "    (\"I-neutral\", \"B-negative\"):0,\n",
    "    (\"I-neutral\", \"B-neutral\"):0,\n",
    "    (\"I-neutral\", \"I-positive\"):0,\n",
    "    (\"I-neutral\", \"I-negative\"):0,\n",
    "    (\"I-neutral\", \"I-neutral\"):0,\n",
    "    (\"I-neutral\", \"START\"):0,\n",
    "    (\"I-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"START\", \"O\"):0,\n",
    "    (\"START\", \"B-positive\"):0,\n",
    "    (\"START\", \"B-negative\"):0,\n",
    "    (\"START\", \"B-neutral\"):0,\n",
    "    (\"START\", \"I-positive\"):0,\n",
    "    (\"START\", \"I-negative\"):0,\n",
    "    (\"START\", \"I-neutral\"):0,\n",
    "    (\"START\", \"START\"):0,\n",
    "    (\"START\", \"STOP\"):0,\n",
    "    \n",
    "    (\"STOP\", \"O\"):0,\n",
    "    (\"STOP\", \"B-positive\"):0,\n",
    "    (\"STOP\", \"B-negative\"):0,\n",
    "    (\"STOP\", \"B-neutral\"):0,\n",
    "    (\"STOP\", \"I-positive\"):0,\n",
    "    (\"STOP\", \"I-negative\"):0,\n",
    "    (\"STOP\", \"I-neutral\"):0,\n",
    "    (\"STOP\", \"START\"):0,\n",
    "    (\"STOP\", \"STOP\"):0,\n",
    "               \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate number of u (init state) to v (next state) transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assumptions made: \n",
    "\n",
    "1. Everything is in tuple pairs except the word \"STOP\" which signifies the end of a review\n",
    "2. Transition probabilities from \"STOP\" to any other states are 0 but still included in the dictionary as required by question\n",
    "\n",
    "\n",
    "\n",
    "Variables:\n",
    "1. u --> initial state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "2. v --> next state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "3. hasbreak --> signifies the end of a review (boolean)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_transition(data, transit_dict):\n",
    "    updated_dict = transit_dict.copy()\n",
    "    hasbreak = True\n",
    "    for word in data:\n",
    "        if ( hasbreak == True ):\n",
    "            u = \"START\"\n",
    "            \n",
    "        if ( word != \"STOP\" and (hasbreak == True) ):\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = False\n",
    "            \n",
    "        elif ( word != \"STOP\" and (hasbreak == False) ):\n",
    "            u = v\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "        \n",
    "        elif ( word == \"STOP\" ):\n",
    "            u = v\n",
    "            v = \"STOP\"\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = True\n",
    "            \n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update for ES and RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_updated_tdict = calculate_transition(EStrain_tup, transit_dict)\n",
    "RU_updated_tdict = calculate_transition(RUtrain_tup, transit_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert above calculation to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_param(updated_tdict):\n",
    "    all_states = ['O', 'B-positive', 'B-negative', 'B-neutral', 'I-positive', 'I-negative', 'I-neutral', 'START', 'STOP']\n",
    "    q_dict = {}\n",
    "    temp_sum = 0\n",
    "    for state in all_states:\n",
    "        total = sum(value for key, value in updated_tdict.items() if key[1] == state)\n",
    "        for key, value in updated_tdict.items():\n",
    "            if ( (key[1] == state) and ( total != 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = value/total\n",
    "            elif ( (key[1] == state) and ( total == 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = 0\n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Below is the final parameters to be used*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Format is v|u (next state given current state)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_transit = transition_param(ES_updated_tdict)\n",
    "RU_transit = transition_param(RU_updated_tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting emission params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_emission_parameter, ES_emission_count, ES_label_count = train_estimate_emission_parameters(ES_train)\n",
    "ES_emission = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "\n",
    "RU_emission_parameter, RU_emission_count, RU_label_count = train_estimate_emission_parameters(RU_train)\n",
    "RU_emission = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dev.in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "ESdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "ESdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in ESdev ]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in RUdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructuring dev.in to list of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ls(dev_in_ls, transit_param):\n",
    "    sequence_list = []\n",
    "    sequence_xi = []\n",
    "    for word in dev_in_ls:\n",
    "        if ( word != \"STOP\"):\n",
    "            sequence_xi.append(word)\n",
    "        else:\n",
    "            sequence_list.append(sequence_xi.copy())\n",
    "            sequence_xi = []\n",
    "    return sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_seq_list = to_ls(ESdev_ls, ES_transit)\n",
    "RU_seq_list = to_ls(RUdev_ls, RU_transit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on previous state, get the possible next states based on the current word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_states(prev_state, desired_word, emit, transit):\n",
    "    filtered_keys = [key for key in emit.keys() if (key[0] == desired_word)]\n",
    "    signal = False\n",
    "    if ( len(filtered_keys) == 0 ):\n",
    "        desired_pattern = f\"|{prev_state}\"\n",
    "        unk = [(key, value) for key, value in transit.items() if ( key.endswith(desired_pattern) and not key.startswith(\"STOP\") and not key.startswith(\"START\") and value!=0 )]\n",
    "        filtered_keys = [(desired_word, item[0].split('|')[0]) for item in unk]\n",
    "        signal = True\n",
    "    possible_states = [key[1] for key in filtered_keys]\n",
    "    return possible_states, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sequence, emit, transit, u, v, index, dt = {}):\n",
    "    u = v\n",
    "    dt[(0,v)] = 0\n",
    "    max_u_ls = []\n",
    "    possible_states = [\"START\"]\n",
    "    word = \"\"\n",
    "    for i in range(len(sequence)):\n",
    "        save_state = possible_states\n",
    "        word = sequence[index]\n",
    "        signal = False\n",
    "        possible_states, signal = get_possible_states(save_state[0], word, emit, transit)\n",
    "        for u in save_state:\n",
    "            for v in possible_states:\n",
    "                a = np.log(transit[f\"{v}|{u}\"])\n",
    "                if signal == True:\n",
    "                    b = np.log(emit[('#UNK#', v)])\n",
    "                else:\n",
    "                    b = np.log(emit[(word, v)])\n",
    "                max_u_ls.append(dt[(index, u)] + a + b)\n",
    "            for j in range(len(possible_states)):\n",
    "                key = ( index+1, possible_states[j])\n",
    "                if key not in dt:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "                elif dt[key] < max_u_ls[j]:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "            max_u_ls = max_u_ls[len(possible_states):]\n",
    "        max_u_ls = []\n",
    "        index += 1\n",
    "    dct = dt.copy()\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(emit, transit, seq_list):\n",
    "    dt = {}\n",
    "    pred = []\n",
    "    for i in seq_list:\n",
    "        temp = viterbi(i, emit, transit, \"START\", \"START\", 0, {})\n",
    "        pred.append(temp)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Might have divide by zero error but it's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n"
     ]
    }
   ],
   "source": [
    "ES_pred = predict(ES_emission, ES_transit, ES_seq_list)\n",
    "RU_pred = predict(RU_emission, RU_transit, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each duplicated index, look for the one with highest log value a.k.a highest prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pred(pred):\n",
    "    pred_ls = []\n",
    "    grouped_dict = {}\n",
    "    for seq in pred:\n",
    "        for key, value in seq.items():\n",
    "            var = key[0]\n",
    "            if var in range(0, len(seq)):\n",
    "                if var not in grouped_dict or value > grouped_dict[var][1]:\n",
    "                    grouped_dict[var] = (key, value)\n",
    "\n",
    "        pred_ls.append([item[0] for item in grouped_dict.values()])\n",
    "        grouped_dict = {}\n",
    "    return pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_proc = processing_pred(ES_pred)\n",
    "RU_proc = processing_pred(RU_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(pred_res, seq_list):\n",
    "    res = pd.DataFrame(columns=['word', 'label'])\n",
    "    for seq_i in range(len(seq_list)):\n",
    "        labels = [label for _, label in pred_res[seq_i][1:]]\n",
    "        df = pd.DataFrame({\n",
    "            'word': seq_list[seq_i],\n",
    "            'label': labels\n",
    "        })\n",
    "        df.loc[len(df)] = ''\n",
    "        res = pd.concat([res, df])\n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2 = consolidate(ES_proc, ES_seq_list)\n",
    "RU_pred_q2 = consolidate(RU_proc, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2.to_csv(\"./ES/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_q2.to_csv(\"./RU/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1014\n",
      "\n",
      "#Correct Entity : 139\n",
      "Entity  precision: 0.1371\n",
      "Entity  recall: 0.6070\n",
      "Entity  F: 0.2237\n",
      "\n",
      "#Correct Sentiment : 86\n",
      "Sentiment  precision: 0.0848\n",
      "Sentiment  recall: 0.3755\n",
      "Sentiment  F: 0.1384\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1392\n",
      "\n",
      "#Correct Entity : 234\n",
      "Entity  precision: 0.1681\n",
      "Entity  recall: 0.6015\n",
      "Entity  F: 0.2628\n",
      "\n",
      "#Correct Sentiment : 150\n",
      "Sentiment  precision: 0.1078\n",
      "Sentiment  recall: 0.3856\n",
      "Sentiment  F: 0.1684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command2 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result2.stdout.decode())\n",
    "\n",
    "command2 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result2.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be modifying the viterbi algorithm by varying the K, obtaining the best k-th sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    r1 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r1]\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "\n",
    "with open('./ES/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r3 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r3]\n",
    "\n",
    "\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    r4 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r4]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r5 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r5]\n",
    "\n",
    "with open('./RU/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r6 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r6]\n",
    "\n",
    "ES_train = pd.DataFrame(data = ES_train_tup, columns = [\"word\", \"label\"])\n",
    "ES_dev_in = pd.DataFrame(data = ES_dev_in_tup, columns = [\"word\"])\n",
    "ES_dev_out = pd.DataFrame(data = ES_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "RU_train = pd.DataFrame(data = RU_train_tup, columns = [\"word\", \"label\"])\n",
    "RU_dev_in = pd.DataFrame(data = RU_dev_in_tup, columns = [\"word\"])\n",
    "RU_dev_out = pd.DataFrame(data = RU_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "ESdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "ESdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in ESdev ]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in RUdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these values emmision and transition parameters, we will proceed to count the sequence by repeating the viterbi algortihm we obtained in q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ES_emission_parameter, ES_emission_count, ES_label_count = train_estimate_emission_parameters(ES_train)\n",
    "ES_emission_2nd = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "ES_emission_8th = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "\n",
    "RU_emission_parameter, RU_emission_count, RU_label_count = train_estimate_emission_parameters(RU_train)\n",
    "RU_emission_2nd = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)\n",
    "RU_emission_8th = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)\n",
    "\n",
    "ES_transit = transition_param(ES_updated_tdict)\n",
    "RU_transit = transition_param(RU_updated_tdict)\n",
    "\n",
    "ES_seq_list = to_ls(ESdev_ls, ES_transit)\n",
    "RU_seq_list = to_ls(RUdev_ls, RU_transit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will modifying the processing_pred function to take in an argument k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def mod_processing_pred(pred, k):\n",
    "    all_pred_ls = []\n",
    "    for seq in pred:\n",
    "        # Initialize priority queue with the highest value sequence\n",
    "        queue = []\n",
    "        grouped_dict = {}\n",
    "        for key, value in seq.items():\n",
    "            var = key[0]\n",
    "            if var in range(0, len(seq)):\n",
    "                if var not in grouped_dict or value > grouped_dict[var][1]:\n",
    "                    grouped_dict[var] = (key, value)\n",
    "        initial_sequence = [item[0] for item in grouped_dict.values()]\n",
    "        initial_value = sum(item[1] for item in grouped_dict.values())\n",
    "        heapq.heappush(queue, (-initial_value, initial_sequence))\n",
    "\n",
    "        # Generate top 10 sequences to limit memory\n",
    "        pred_ls = []\n",
    "        for _ in range(10):\n",
    "            if not queue:\n",
    "                break\n",
    "            value, sequence = heapq.heappop(queue)\n",
    "            pred_ls.append(sequence)\n",
    "\n",
    "            # Generate new sequences by replacing one element at a time\n",
    "            for i in range(len(sequence)):\n",
    "                replaced_key = sequence[i]\n",
    "                remaining_keys = [key for key in seq.keys() if key[0] == replaced_key[0] and key != replaced_key]\n",
    "                for new_key in remaining_keys:\n",
    "                    new_sequence = sequence[:i] + [new_key] + sequence[i+1:]\n",
    "                    new_value = value - seq[replaced_key] + seq[new_key]\n",
    "                    heapq.heappush(queue, (-new_value, new_sequence))\n",
    "\n",
    "        all_pred_ls.append(pred_ls)\n",
    "\n",
    "    kth_sequences = [seq[min(k-1, len(seq)-1)] for seq in all_pred_ls ]\n",
    "\n",
    "    \n",
    "    return kth_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n",
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\1238102201.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_value = value - seq[replaced_key] + seq[new_key]\n"
     ]
    }
   ],
   "source": [
    "ES_pred_2nd = predict(ES_emission_2nd, ES_transit, ES_seq_list)\n",
    "RU_pred_2nd = predict(RU_emission_2nd, RU_transit, RU_seq_list)\n",
    "\n",
    "ES_proc_2nd = mod_processing_pred(ES_pred_2nd, k=2)\n",
    "RU_proc_2nd = mod_processing_pred(RU_pred_2nd, k=2)\n",
    "\n",
    "ES_pred_k2 = consolidate(ES_proc_2nd, ES_seq_list)\n",
    "RU_pred_k2 = consolidate(RU_proc_2nd, RU_seq_list)\n",
    "\n",
    "ES_pred_k2.to_csv(\"./ES/dev.p3.2nd.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_k2.to_csv(\"./RU/dev.p3.2nd.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n",
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\1238102201.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_value = value - seq[replaced_key] + seq[new_key]\n"
     ]
    }
   ],
   "source": [
    "ES_pred_8th = predict(ES_emission_8th, ES_transit, ES_seq_list)\n",
    "RU_pred_8th = predict(RU_emission_8th, RU_transit, RU_seq_list)\n",
    "\n",
    "ES_proc_8th = mod_processing_pred(ES_pred_8th, 8)\n",
    "RU_proc_8th = mod_processing_pred(RU_pred_8th, 8)\n",
    "\n",
    "ES_pred_k8 = consolidate(ES_proc_8th, ES_seq_list)\n",
    "RU_pred_k8 = consolidate(RU_proc_8th, RU_seq_list)\n",
    "\n",
    "ES_pred_k8.to_csv(\"./ES/dev.p3.8th.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_k8.to_csv(\"./RU/dev.p3.8th.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1044\n",
      "\n",
      "#Correct Entity : 139\n",
      "Entity  precision: 0.1331\n",
      "Entity  recall: 0.6070\n",
      "Entity  F: 0.2184\n",
      "\n",
      "#Correct Sentiment : 69\n",
      "Sentiment  precision: 0.0661\n",
      "Sentiment  recall: 0.3013\n",
      "Sentiment  F: 0.1084\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1424\n",
      "\n",
      "#Correct Entity : 230\n",
      "Entity  precision: 0.1615\n",
      "Entity  recall: 0.5913\n",
      "Entity  F: 0.2537\n",
      "\n",
      "#Correct Sentiment : 136\n",
      "Sentiment  precision: 0.0955\n",
      "Sentiment  recall: 0.3496\n",
      "Sentiment  F: 0.1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command3 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'./ES/dev.p3.2nd.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result3.stdout.decode())\n",
    "\n",
    "command3 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'./RU/dev.p3.2nd.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result3.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1061\n",
      "\n",
      "#Correct Entity : 141\n",
      "Entity  precision: 0.1329\n",
      "Entity  recall: 0.6157\n",
      "Entity  F: 0.2186\n",
      "\n",
      "#Correct Sentiment : 75\n",
      "Sentiment  precision: 0.0707\n",
      "Sentiment  recall: 0.3275\n",
      "Sentiment  F: 0.1163\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1408\n",
      "\n",
      "#Correct Entity : 213\n",
      "Entity  precision: 0.1513\n",
      "Entity  recall: 0.5476\n",
      "Entity  F: 0.2371\n",
      "\n",
      "#Correct Sentiment : 116\n",
      "Sentiment  precision: 0.0824\n",
      "Sentiment  recall: 0.2982\n",
      "Sentiment  F: 0.1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command3 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'./ES/dev.p3.8th.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result3.stdout.decode())\n",
    "\n",
    "command3 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'./RU/dev.p3.8th.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result3.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To improve on our results, we look at possible reasons as to why our previous scores are low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We found that the emission probability is very low since we are looking at the probability of the word given the state, which is very low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given that there are a lot of state 'O's, P(word|\"O\") will be low. In addition, if we look at each particular word in the training set, most of them should have label \"O\". Hence if we look at P(word|\"O\"), it will most likely be the lowest out of all the other states due to number of state \"O\"s being a lot greater, diluting the probability. In other words, we end up predicting the state of the word to be B-neg, B-pos, I-pos etc. much, much more than predicting the state of the word to be \"O\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To solve this issue, we decided to look at the probability of the state given the word, rather than the word given the state. We first modify two functions for our emission parameter generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimate_emission_parameters_word(train):\n",
    "    word_count = {}\n",
    "    emission_count = {}\n",
    "    label_count = {}\n",
    "\n",
    "    for row in range(train.shape[0]):\n",
    "        word = train.iloc[row, 0]\n",
    "        label = train.iloc[row, 1]\n",
    "        if (word not in word_count) and (word != \"null\"):\n",
    "            word_count[word] = 1\n",
    "        elif ( word != \"null\" ):\n",
    "            word_count[word] += 1\n",
    "        \n",
    "        if (label not in label_count) and (label != \"null\"):\n",
    "            label_count[label] = 1\n",
    "        elif (label != \"null\"):\n",
    "            label_count[label] += 1\n",
    "\n",
    "        if ((word, label) not in emission_count) and (word != \"null\"):\n",
    "            emission_count[(word, label)] = 1\n",
    "        elif (word != \"null\"):\n",
    "            emission_count[(word, label)] += 1\n",
    "\n",
    "    emission_parameter = {}\n",
    "    for word, label in emission_count:\n",
    "        emission_parameter[(word, label)] = emission_count[(word, label)] / word_count[word]\n",
    "\n",
    "    return emission_parameter, emission_count, label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgraded_estimate_emission_parameters_word(test_set, emission_parameter, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 0\n",
    "    special_word = '#UNK#'\n",
    "    test_copy = test_set.copy()\n",
    "    #train word list is all the first argument in keys of emission counts\n",
    "    train_word_list= [word for word, label in emission_counts.keys()]\n",
    "    for i in range(len(test_copy[\"word\"])):\n",
    "        word = test_copy[\"word\"][i]\n",
    "        if word not in train_word_list:\n",
    "            test_copy[\"word\"][i] = special_word\n",
    "            k+=1\n",
    "\n",
    "    for word in test_copy[\"word\"]:\n",
    "        for label in label_counts:\n",
    "            if (word == special_word) and ((word, label) not in emission_parameter):\n",
    "                emission_parameter[(word, label)] = (k*(label_counts.get(label)/sum(label_counts.values())))/k\n",
    "                # print((word, label), emission_parameter.get((word, label)))\n",
    "                      \n",
    "    return emission_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We train our values based on the new functions, similar to q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_ES_emit, q4_ES_emitcount, q4_ES_label_count = train_estimate_emission_parameters_word(ES_train)\n",
    "q4_ES_emit_fin = upgraded_estimate_emission_parameters_word(ES_dev_in, q4_ES_emit, q4_ES_emitcount, q4_ES_label_count)\n",
    "\n",
    "q4_RU_emit, q4_RU_emitcount, q4_RU_label_count = train_estimate_emission_parameters_word(RU_train)\n",
    "q4_RU_emit_fin = upgraded_estimate_emission_parameters_word(RU_dev_in, q4_RU_emit, q4_RU_emitcount, q4_RU_label_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE predict using the function in q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n"
     ]
    }
   ],
   "source": [
    "ES_pred_q4 = predict(q4_ES_emit_fin, ES_transit, ES_seq_list)\n",
    "RU_pred_q4 = predict(q4_RU_emit_fin, RU_transit, RU_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_proc_q4 = processing_pred(ES_pred_q4)\n",
    "RU_proc_q4 = processing_pred(RU_pred_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q4 = consolidate(ES_proc_q4, ES_seq_list)\n",
    "RU_pred_q4 = consolidate(RU_proc_q4, RU_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q4.to_csv(\"./ES/dev.p4.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_q4.to_csv(\"./RU/dev.p4.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 212\n",
      "\n",
      "#Correct Entity : 131\n",
      "Entity  precision: 0.6179\n",
      "Entity  recall: 0.5721\n",
      "Entity  F: 0.5941\n",
      "\n",
      "#Correct Sentiment : 98\n",
      "Sentiment  precision: 0.4623\n",
      "Sentiment  recall: 0.4279\n",
      "Sentiment  F: 0.4444\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 276\n",
      "\n",
      "#Correct Entity : 181\n",
      "Entity  precision: 0.6558\n",
      "Entity  recall: 0.4653\n",
      "Entity  F: 0.5444\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.4674\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.3880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command4 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p4.out']\n",
    "result4 = subprocess.run(command4, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result4.stdout.decode())\n",
    "\n",
    "command4 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p4.out']\n",
    "result4 = subprocess.run(command4, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result4.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It would seem that the results are a lot better, based on just making sense of our training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
