{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group THE BOYS 1D ML project\n",
    "1. Loy Pek Yong 1004475\n",
    "2. Lim Yongqing 1005955\n",
    "3. Sim Reynard Simano 1006307\n",
    "\n",
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries + reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "\n",
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    r1 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r1]\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "\n",
    "with open('./ES/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r3 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r3]\n",
    "\n",
    "\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    r4 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r4]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r5 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r5]\n",
    "\n",
    "with open('./RU/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r6 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_train = pd.DataFrame(data = ES_train_tup, columns = [\"word\", \"label\"])\n",
    "ES_dev_in = pd.DataFrame(data = ES_dev_in_tup, columns = [\"word\"])\n",
    "ES_dev_out = pd.DataFrame(data = ES_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "RU_train = pd.DataFrame(data = RU_train_tup, columns = [\"word\", \"label\"])\n",
    "RU_dev_in = pd.DataFrame(data = RU_dev_in_tup, columns = [\"word\"])\n",
    "RU_dev_out = pd.DataFrame(data = RU_dev_out_tup, columns = [\"word\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimate_emission_parameters(train):\n",
    "    # word_column = train[\"word\"]\n",
    "    # label_column = train[\"label\"]\n",
    "\n",
    "    label_count = {}\n",
    "    emission_count = {}\n",
    "\n",
    "    for row in range(train.shape[0]):\n",
    "        word = train.iloc[row, 0]\n",
    "        label = train.iloc[row, 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "\n",
    "        if (word, label) not in emission_count:\n",
    "            emission_count[(word, label)] = 1\n",
    "        else:\n",
    "            emission_count[(word, label)] += 1\n",
    "\n",
    "    emission_parameter = {}\n",
    "    for word, label in emission_count:\n",
    "        emission_parameter[(word, label)] = emission_count[(word, label)] / label_count[label]\n",
    "\n",
    "    return emission_parameter, emission_count, label_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 0\n",
    "    special_word = '#UNK#'\n",
    "    test_copy = test_set.copy()\n",
    "    #train word list is all the first argument in keys of emission counts\n",
    "    train_word_list= [word for word, label in emission_counts.keys()]\n",
    "    for i in range(len(test_copy[\"word\"])):\n",
    "        word = test_copy[\"word\"][i]\n",
    "        if word not in train_word_list:\n",
    "            test_copy[\"word\"][i] = special_word\n",
    "            k+=1\n",
    "\n",
    "    for word in test_copy[\"word\"]:\n",
    "        for label in label_counts:\n",
    "            if word == special_word:\n",
    "                emission_parameter[(word, label)] = k / (label_counts[label] + k)\n",
    "            elif ( (word, label) in emission_parameter ):\n",
    "                emission_parameter[(word, label)] = emission_counts[(word, label)] / (label_counts[label] + k)\n",
    "                \n",
    "    # print(emission_parameter)        \n",
    "    return emission_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 of q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(training_set, test_set):\n",
    "    emission_parameter, emission_count, label_count = train_estimate_emission_parameters(training_set)\n",
    "    emission_parameter = upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_count, label_count)\n",
    "    test_set[\"label\"] = \"\"\n",
    "\n",
    "    #label equals to argmax of emission parameter\n",
    "    for row in range(len(test_set)):\n",
    "        word = test_set.iloc[row][\"word\"]\n",
    "        max_val = 0\n",
    "        max_label = \"\"\n",
    "        train_word_list = [word for word, label in emission_count.keys()]\n",
    "        \n",
    "        for label in label_count:\n",
    "            if (word, label) in emission_parameter:\n",
    "                if emission_parameter[(word, label)] > max_val:\n",
    "                    max_val = emission_parameter[(word, label)]\n",
    "                    max_label = label\n",
    "                    \n",
    "            else:\n",
    "                if word not in train_word_list:\n",
    "                    # get max label from word #UNK#\n",
    "                    if (\"#UNK#\", label) in emission_parameter:\n",
    "                        if emission_parameter[(\"#UNK#\", label)] > max_val:\n",
    "                            max_val = emission_parameter[(\"#UNK#\", label)]\n",
    "                            max_label = label\n",
    "        test_set.loc[row, \"label\"] = max_label\n",
    "                    \n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting + saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_dev_p1_out = sentiment_analysis(ES_train, ES_dev_in)\n",
    "ES_dev_p1_out_r = ES_dev_p1_out.replace('null', pd.NA)\n",
    "ES_dev_p1_out_r.to_csv(\"./ES/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "    \n",
    "RU_dev_p1_out = sentiment_analysis(RU_train, RU_dev_in)\n",
    "RU_dev_p1_out_r = RU_dev_p1_out.replace('null', pd.NA)\n",
    "RU_dev_p1_out_r.to_csv(\"./RU/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1144\n",
      "\n",
      "#Correct Entity : 181\n",
      "Entity  precision: 0.1582\n",
      "Entity  recall: 0.7904\n",
      "Entity  F: 0.2637\n",
      "\n",
      "#Correct Sentiment : 120\n",
      "Sentiment  precision: 0.1049\n",
      "Sentiment  recall: 0.5240\n",
      "Sentiment  F: 0.1748\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1467\n",
      "\n",
      "#Correct Entity : 288\n",
      "Entity  precision: 0.1963\n",
      "Entity  recall: 0.7404\n",
      "Entity  F: 0.3103\n",
      "\n",
      "#Correct Sentiment : 174\n",
      "Sentiment  precision: 0.1186\n",
      "Sentiment  recall: 0.4473\n",
      "Sentiment  F: 0.1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result.stdout.decode())\n",
    "\n",
    "command = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "EStrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "EStrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in EStrain ]\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUtrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUtrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in RUtrain ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reference transition dictionary (all transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_dict = {\n",
    "    (\"O\", \"O\"): 0,\n",
    "    (\"O\", \"B-positive\"):0,\n",
    "    (\"O\", \"B-negative\"):0,\n",
    "    (\"O\", \"B-neutral\"):0,\n",
    "    (\"O\", \"I-positive\"):0,\n",
    "    (\"O\", \"I-negative\"):0,\n",
    "    (\"O\", \"I-neutral\"):0,\n",
    "    (\"O\", \"START\"):0,\n",
    "    (\"O\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-positive\", \"O\"):0,\n",
    "    (\"B-positive\", \"B-positive\"):0,\n",
    "    (\"B-positive\", \"B-negative\"):0,\n",
    "    (\"B-positive\", \"B-neutral\"):0,\n",
    "    (\"B-positive\", \"I-positive\"):0,\n",
    "    (\"B-positive\", \"I-negative\"):0,\n",
    "    (\"B-positive\", \"I-neutral\"):0,\n",
    "    (\"B-positive\", \"START\"):0,\n",
    "    (\"B-positive\", \"STOP\"):0,\n",
    "    \n",
    "    \n",
    "    (\"B-negative\", \"O\"):0,\n",
    "    (\"B-negative\", \"B-positive\"):0,\n",
    "    (\"B-negative\", \"B-negative\"):0,\n",
    "    (\"B-negative\", \"B-neutral\"):0,\n",
    "    (\"B-negative\", \"I-positive\"):0,\n",
    "    (\"B-negative\", \"I-negative\"):0,\n",
    "    (\"B-negative\", \"I-neutral\"):0,\n",
    "    (\"B-negative\", \"START\"):0,\n",
    "    (\"B-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-neutral\", \"O\"):0,\n",
    "    (\"B-neutral\", \"B-positive\"):0,\n",
    "    (\"B-neutral\", \"B-negative\"):0,\n",
    "    (\"B-neutral\", \"B-neutral\"):0,\n",
    "    (\"B-neutral\", \"I-positive\"):0,\n",
    "    (\"B-neutral\", \"I-negative\"):0,\n",
    "    (\"B-neutral\", \"I-neutral\"):0,\n",
    "    (\"B-neutral\", \"START\"):0,\n",
    "    (\"B-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-positive\", \"O\"):0,\n",
    "    (\"I-positive\", \"B-positive\"):0,\n",
    "    (\"I-positive\", \"B-negative\"):0,\n",
    "    (\"I-positive\", \"B-neutral\"):0,\n",
    "    (\"I-positive\", \"I-positive\"):0,\n",
    "    (\"I-positive\", \"I-negative\"):0,\n",
    "    (\"I-positive\", \"I-neutral\"):0,\n",
    "    (\"I-positive\", \"START\"):0,\n",
    "    (\"I-positive\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-negative\", \"O\"):0,\n",
    "    (\"I-negative\", \"B-positive\"):0,\n",
    "    (\"I-negative\", \"B-negative\"):0,\n",
    "    (\"I-negative\", \"B-neutral\"):0,\n",
    "    (\"I-negative\", \"I-positive\"):0,\n",
    "    (\"I-negative\", \"I-negative\"):0,\n",
    "    (\"I-negative\", \"I-neutral\"):0,\n",
    "    (\"I-negative\", \"START\"):0,\n",
    "    (\"I-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-neutral\", \"O\"):0,\n",
    "    (\"I-neutral\", \"B-positive\"):0,\n",
    "    (\"I-neutral\", \"B-negative\"):0,\n",
    "    (\"I-neutral\", \"B-neutral\"):0,\n",
    "    (\"I-neutral\", \"I-positive\"):0,\n",
    "    (\"I-neutral\", \"I-negative\"):0,\n",
    "    (\"I-neutral\", \"I-neutral\"):0,\n",
    "    (\"I-neutral\", \"START\"):0,\n",
    "    (\"I-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"START\", \"O\"):0,\n",
    "    (\"START\", \"B-positive\"):0,\n",
    "    (\"START\", \"B-negative\"):0,\n",
    "    (\"START\", \"B-neutral\"):0,\n",
    "    (\"START\", \"I-positive\"):0,\n",
    "    (\"START\", \"I-negative\"):0,\n",
    "    (\"START\", \"I-neutral\"):0,\n",
    "    (\"START\", \"START\"):0,\n",
    "    (\"START\", \"STOP\"):0,\n",
    "    \n",
    "    (\"STOP\", \"O\"):0,\n",
    "    (\"STOP\", \"B-positive\"):0,\n",
    "    (\"STOP\", \"B-negative\"):0,\n",
    "    (\"STOP\", \"B-neutral\"):0,\n",
    "    (\"STOP\", \"I-positive\"):0,\n",
    "    (\"STOP\", \"I-negative\"):0,\n",
    "    (\"STOP\", \"I-neutral\"):0,\n",
    "    (\"STOP\", \"START\"):0,\n",
    "    (\"STOP\", \"STOP\"):0,\n",
    "               \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate number of u (init state) to v (next state) transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assumptions made: \n",
    "\n",
    "1. Everything is in tuple pairs except the word \"STOP\" which signifies the end of a review\n",
    "2. Transition probabilities from \"STOP\" to any other states are 0 but still included in the dictionary as required by question\n",
    "\n",
    "\n",
    "\n",
    "Variables:\n",
    "1. u --> initial state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "2. v --> next state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "3. hasbreak --> signifies the end of a review (boolean)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_transition(data, transit_dict):\n",
    "    updated_dict = transit_dict.copy()\n",
    "    hasbreak = True\n",
    "    for word in data:\n",
    "        if ( hasbreak == True ):\n",
    "            u = \"START\"\n",
    "            \n",
    "        if ( word != \"STOP\" and (hasbreak == True) ):\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = False\n",
    "            \n",
    "        elif ( word != \"STOP\" and (hasbreak == False) ):\n",
    "            u = v\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "        \n",
    "        elif ( word == \"STOP\" ):\n",
    "            u = v\n",
    "            v = \"STOP\"\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = True\n",
    "            \n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update for ES and RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_updated_tdict = calculate_transition(EStrain_tup, transit_dict)\n",
    "RU_updated_tdict = calculate_transition(RUtrain_tup, transit_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert above calculation to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_param(updated_tdict):\n",
    "    all_states = ['O', 'B-positive', 'B-negative', 'B-neutral', 'I-positive', 'I-negative', 'I-neutral', 'START', 'STOP']\n",
    "    q_dict = {}\n",
    "    temp_sum = 0\n",
    "    for state in all_states:\n",
    "        total = sum(value for key, value in updated_tdict.items() if key[1] == state)\n",
    "        for key, value in updated_tdict.items():\n",
    "            if ( (key[1] == state) and ( total != 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = value/total\n",
    "            elif ( (key[1] == state) and ( total == 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = 0\n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Below is the final parameters to be used*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Format is v|u (next state given current state)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_transit = transition_param(ES_updated_tdict)\n",
    "RU_transit = transition_param(RU_updated_tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting emission params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_emission_parameter, ES_emission_count, ES_label_count = train_estimate_emission_parameters(ES_train)\n",
    "ES_emission = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "\n",
    "RU_emission_parameter, RU_emission_count, RU_label_count = train_estimate_emission_parameters(RU_train)\n",
    "RU_emission = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dev.in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "ESdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "ESdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in ESdev ]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in RUdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructuring dev.in to list of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ls(dev_in_ls, transit_param):\n",
    "    sequence_list = []\n",
    "    sequence_xi = []\n",
    "    for word in dev_in_ls:\n",
    "        if ( word != \"STOP\"):\n",
    "            sequence_xi.append(word)\n",
    "        else:\n",
    "            sequence_list.append(sequence_xi.copy())\n",
    "            sequence_xi = []\n",
    "    return sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_seq_list = to_ls(ESdev_ls, ES_transit)\n",
    "RU_seq_list = to_ls(RUdev_ls, RU_transit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on previous state, get the possible next states based on the current word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_states(prev_state, desired_word, emit, transit):\n",
    "    filtered_keys = [key for key in emit.keys() if (key[0] == desired_word)]\n",
    "    signal = False\n",
    "    if ( len(filtered_keys) == 0 ):\n",
    "        desired_pattern = f\"|{prev_state}\"\n",
    "        unk = [(key, value) for key, value in transit.items() if ( key.endswith(desired_pattern) and not key.startswith(\"STOP\") and not key.startswith(\"START\") and value!=0 )]\n",
    "        filtered_keys = [(desired_word, item[0].split('|')[0]) for item in unk]\n",
    "        signal = True\n",
    "    possible_states = [key[1] for key in filtered_keys]\n",
    "    return possible_states, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sequence, emit, transit, u, v, index, dt = {}):\n",
    "    u = v\n",
    "    dt[(0,v)] = 0\n",
    "    max_u_ls = []\n",
    "    possible_states = [\"START\"]\n",
    "    word = \"\"\n",
    "    for i in range(len(sequence)):\n",
    "        save_state = possible_states\n",
    "        word = sequence[index]\n",
    "        signal = False\n",
    "        possible_states, signal = get_possible_states(save_state[0], word, emit, transit)\n",
    "        for u in save_state:\n",
    "            for v in possible_states:\n",
    "                a = np.log(transit[f\"{v}|{u}\"])\n",
    "                if signal == True:\n",
    "                    b = np.log(emit[('#UNK#', v)])\n",
    "                else:\n",
    "                    b = np.log(emit[(word, v)])\n",
    "                max_u_ls.append(dt[(index, u)] + a + b)\n",
    "            for j in range(len(possible_states)):\n",
    "                key = ( index+1, possible_states[j])\n",
    "                if key not in dt:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "                elif dt[key] < max_u_ls[j]:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "            max_u_ls = max_u_ls[len(possible_states):]\n",
    "        max_u_ls = []\n",
    "        index += 1\n",
    "    dct = dt.copy()\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(emit, transit, seq_list):\n",
    "    dt = {}\n",
    "    pred = []\n",
    "    for i in seq_list:\n",
    "        temp = viterbi(i, emit, transit, \"START\", \"START\", 0, {})\n",
    "        pred.append(temp)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Might have divide by zero error but it's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n"
     ]
    }
   ],
   "source": [
    "ES_pred = predict(ES_emission, ES_transit, ES_seq_list)\n",
    "RU_pred = predict(RU_emission, RU_transit, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each duplicated index, look for the one with highest log value a.k.a highest prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pred(pred):\n",
    "    pred_ls = []\n",
    "    grouped_dict = {}\n",
    "    for seq in pred:\n",
    "        for key, value in seq.items():\n",
    "            var = key[0]\n",
    "            if var in range(0, len(seq)):\n",
    "                if var not in grouped_dict or value > grouped_dict[var][1]:\n",
    "                    grouped_dict[var] = (key, value)\n",
    "\n",
    "        pred_ls.append([item[0] for item in grouped_dict.values()])\n",
    "        grouped_dict = {}\n",
    "    return pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_proc = processing_pred(ES_pred)\n",
    "RU_proc = processing_pred(RU_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(pred_res, seq_list):\n",
    "    res = pd.DataFrame(columns=['word', 'label'])\n",
    "    for seq_i in range(len(seq_list)):\n",
    "        labels = [label for _, label in pred_res[seq_i][1:]]\n",
    "        df = pd.DataFrame({\n",
    "            'word': seq_list[seq_i],\n",
    "            'label': labels\n",
    "        })\n",
    "        df.loc[len(df)] = ''\n",
    "        res = pd.concat([res, df])\n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2 = consolidate(ES_proc, ES_seq_list)\n",
    "RU_pred_q2 = consolidate(RU_proc, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2.to_csv(\"./ES/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_q2.to_csv(\"./RU/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1014\n",
      "\n",
      "#Correct Entity : 139\n",
      "Entity  precision: 0.1371\n",
      "Entity  recall: 0.6070\n",
      "Entity  F: 0.2237\n",
      "\n",
      "#Correct Sentiment : 86\n",
      "Sentiment  precision: 0.0848\n",
      "Sentiment  recall: 0.3755\n",
      "Sentiment  F: 0.1384\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1392\n",
      "\n",
      "#Correct Entity : 234\n",
      "Entity  precision: 0.1681\n",
      "Entity  recall: 0.6015\n",
      "Entity  F: 0.2628\n",
      "\n",
      "#Correct Sentiment : 150\n",
      "Sentiment  precision: 0.1078\n",
      "Sentiment  recall: 0.3856\n",
      "Sentiment  F: 0.1684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command2 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result2.stdout.decode())\n",
    "\n",
    "command2 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result2.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be modifying the viterbi algorithm by varying the K, obtaining the best k-th sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    r1 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r1]\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "\n",
    "with open('./ES/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r3 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r3]\n",
    "\n",
    "\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    r4 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r4]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r5 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r5]\n",
    "\n",
    "with open('./RU/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r6 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r6]\n",
    "\n",
    "ES_train = pd.DataFrame(data = ES_train_tup, columns = [\"word\", \"label\"])\n",
    "ES_dev_in = pd.DataFrame(data = ES_dev_in_tup, columns = [\"word\"])\n",
    "ES_dev_out = pd.DataFrame(data = ES_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "RU_train = pd.DataFrame(data = RU_train_tup, columns = [\"word\", \"label\"])\n",
    "RU_dev_in = pd.DataFrame(data = RU_dev_in_tup, columns = [\"word\"])\n",
    "RU_dev_out = pd.DataFrame(data = RU_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "ESdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "ESdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in ESdev ]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in RUdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these values emmision and transition parameters, we will proceed to count the sequence by repeating the viterbi algortihm we obtained in q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ES_emission_parameter, ES_emission_count, ES_label_count = train_estimate_emission_parameters(ES_train)\n",
    "ES_emission_2nd = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "ES_emission_8th = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "\n",
    "RU_emission_parameter, RU_emission_count, RU_label_count = train_estimate_emission_parameters(RU_train)\n",
    "RU_emission_2nd = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)\n",
    "RU_emission_8th = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)\n",
    "\n",
    "ES_transit = transition_param(ES_updated_tdict)\n",
    "RU_transit = transition_param(RU_updated_tdict)\n",
    "\n",
    "ES_seq_list = to_ls(ESdev_ls, ES_transit)\n",
    "RU_seq_list = to_ls(RUdev_ls, RU_transit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will modifying the processing_pred function to take in an argument k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def mod_processing_pred(pred, k):\n",
    "    all_pred_ls = []\n",
    "    for seq in pred:\n",
    "        # Initialize priority queue with the highest value sequence\n",
    "        queue = []\n",
    "        grouped_dict = {}\n",
    "        for key, value in seq.items():\n",
    "            var = key[0]\n",
    "            if var in range(0, len(seq)):\n",
    "                if var not in grouped_dict or value > grouped_dict[var][1]:\n",
    "                    grouped_dict[var] = (key, value)\n",
    "        initial_sequence = [item[0] for item in grouped_dict.values()]\n",
    "        initial_value = sum(item[1] for item in grouped_dict.values())\n",
    "        heapq.heappush(queue, (-initial_value, initial_sequence))\n",
    "\n",
    "        # Generate top 10 sequences to limit memory\n",
    "        pred_ls = []\n",
    "        for _ in range(10):\n",
    "            if not queue:\n",
    "                break\n",
    "            value, sequence = heapq.heappop(queue)\n",
    "            pred_ls.append(sequence)\n",
    "\n",
    "            # Generate new sequences by replacing one element at a time\n",
    "            for i in range(len(sequence)):\n",
    "                replaced_key = sequence[i]\n",
    "                remaining_keys = [key for key in seq.keys() if key[0] == replaced_key[0] and key != replaced_key]\n",
    "                for new_key in remaining_keys:\n",
    "                    new_sequence = sequence[:i] + [new_key] + sequence[i+1:]\n",
    "                    new_value = value - seq[replaced_key] + seq[new_key]\n",
    "                    heapq.heappush(queue, (-new_value, new_sequence))\n",
    "\n",
    "        all_pred_ls.append(pred_ls)\n",
    "\n",
    "    kth_sequences = [seq[min(k-1, len(seq)-1)] for seq in all_pred_ls ]\n",
    "\n",
    "    \n",
    "    return kth_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n",
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\1238102201.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_value = value - seq[replaced_key] + seq[new_key]\n"
     ]
    }
   ],
   "source": [
    "ES_pred_2nd = predict(ES_emission_2nd, ES_transit, ES_seq_list)\n",
    "RU_pred_2nd = predict(RU_emission_2nd, RU_transit, RU_seq_list)\n",
    "\n",
    "ES_proc_2nd = mod_processing_pred(ES_pred_2nd, k=2)\n",
    "RU_proc_2nd = mod_processing_pred(RU_pred_2nd, k=2)\n",
    "\n",
    "ES_pred_k2 = consolidate(ES_proc_2nd, ES_seq_list)\n",
    "RU_pred_k2 = consolidate(RU_proc_2nd, RU_seq_list)\n",
    "\n",
    "ES_pred_k2.to_csv(\"./ES/dev.p3.2nd.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_k2.to_csv(\"./RU/dev.p3.2nd.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n",
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\1238102201.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_value = value - seq[replaced_key] + seq[new_key]\n"
     ]
    }
   ],
   "source": [
    "ES_pred_8th = predict(ES_emission_8th, ES_transit, ES_seq_list)\n",
    "RU_pred_8th = predict(RU_emission_8th, RU_transit, RU_seq_list)\n",
    "\n",
    "ES_proc_8th = mod_processing_pred(ES_pred_8th, 8)\n",
    "RU_proc_8th = mod_processing_pred(RU_pred_8th, 8)\n",
    "\n",
    "ES_pred_k8 = consolidate(ES_proc_8th, ES_seq_list)\n",
    "RU_pred_k8 = consolidate(RU_proc_8th, RU_seq_list)\n",
    "\n",
    "ES_pred_k8.to_csv(\"./ES/dev.p3.8th.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_k8.to_csv(\"./RU/dev.p3.8th.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1044\n",
      "\n",
      "#Correct Entity : 139\n",
      "Entity  precision: 0.1331\n",
      "Entity  recall: 0.6070\n",
      "Entity  F: 0.2184\n",
      "\n",
      "#Correct Sentiment : 69\n",
      "Sentiment  precision: 0.0661\n",
      "Sentiment  recall: 0.3013\n",
      "Sentiment  F: 0.1084\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1424\n",
      "\n",
      "#Correct Entity : 230\n",
      "Entity  precision: 0.1615\n",
      "Entity  recall: 0.5913\n",
      "Entity  F: 0.2537\n",
      "\n",
      "#Correct Sentiment : 136\n",
      "Sentiment  precision: 0.0955\n",
      "Sentiment  recall: 0.3496\n",
      "Sentiment  F: 0.1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command3 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'./ES/dev.p3.2nd.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result3.stdout.decode())\n",
    "\n",
    "command3 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'./RU/dev.p3.2nd.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result3.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1061\n",
      "\n",
      "#Correct Entity : 141\n",
      "Entity  precision: 0.1329\n",
      "Entity  recall: 0.6157\n",
      "Entity  F: 0.2186\n",
      "\n",
      "#Correct Sentiment : 75\n",
      "Sentiment  precision: 0.0707\n",
      "Sentiment  recall: 0.3275\n",
      "Sentiment  F: 0.1163\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1408\n",
      "\n",
      "#Correct Entity : 213\n",
      "Entity  precision: 0.1513\n",
      "Entity  recall: 0.5476\n",
      "Entity  F: 0.2371\n",
      "\n",
      "#Correct Sentiment : 116\n",
      "Sentiment  precision: 0.0824\n",
      "Sentiment  recall: 0.2982\n",
      "Sentiment  F: 0.1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command3 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'./ES/dev.p3.8th.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result3.stdout.decode())\n",
    "\n",
    "command3 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'./RU/dev.p3.8th.out']\n",
    "result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result3.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into sentences by '.', '?', '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_list(lst):\n",
    "#     result = []\n",
    "#     sublist = []\n",
    "#     for item in lst:\n",
    "#         if item[0] == '.' or  item[0] == '?' or  item[0] == '!':\n",
    "#             sublist.append(item)\n",
    "#             result.append(sublist)\n",
    "#             sublist = []\n",
    "#         else:\n",
    "#             sublist.append(item)\n",
    "#     if sublist:\n",
    "#         result.append(sublist)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process ES sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine entries in tuple form\n",
    "# es_sentences_combined = [tuple(x) for x in ES_train.to_records(index=False)]\n",
    "# # Filter out null entries\n",
    "# es_sentences_combined = list(filter(lambda x: x != ('null', 'null'), es_sentences_combined))\n",
    "\n",
    "# es_sentences = split_list(es_sentences_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process RU sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine entries in tuple form\n",
    "# ru_sentences_combined = [tuple(x) for x in RU_train.to_records(index=False)]\n",
    "# # Filter out null entries\n",
    "# ru_sentences_combined = list(filter(lambda x: x != ('null', 'null'), ru_sentences_combined))\n",
    "\n",
    "# ru_sentences = split_list(ru_sentences_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ES features by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spanish\n",
    "# import re\n",
    "\n",
    "# def get_features_es(word):\n",
    "#     features = {}\n",
    "  \n",
    "#     # Check last 3 letters for Spanish verb conjugation\n",
    "#     features['VERB'] = int(word.endswith('o') or word.endswith('as') or word.endswith('a') or word.endswith('amos'))\n",
    "#     # Check last 2 letters for Spanish plurals and gender\n",
    "#     features['PLURAL'] = int(word.endswith('os') or word.endswith('as'))\n",
    "#     features['MASCULINE'] = int(word.endswith('o'))\n",
    "#     features['FEMININE'] = int(word.endswith('a'))\n",
    "\n",
    "#     # Check last 4 letters for Spanish superlative\n",
    "#     features['SUPERLATIVE'] = int(word.endswith('ísimo'))\n",
    "\n",
    "#     # Check for diminutive suffixes\n",
    "#     features['DIMINUTIVE'] = 1 if re.search(r'ito$|ita$|ito$|illa$|ico$|ica$|illo$|illa$', word) else 0\n",
    "    \n",
    "#     # Check for punctuation\n",
    "#     features['has_punctuation'] = int(any(char in word for char in r'!?.,\"-'))\n",
    "    \n",
    "#     return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract RU features by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Russian\n",
    "# def get_features_ru(word):\n",
    "\n",
    "#     features = {}\n",
    "  \n",
    "#     # Check last 2 letters for verb conjugation\n",
    "#     features['INFINITIVE'] = int(word.endswith('ть'))\n",
    "#     features['PAST_MASC'] = int(word.endswith('л'))\n",
    "#     features['PAST_FEM'] = int(word.endswith('ла'))\n",
    "\n",
    "#     # Check last letter for noun gender \n",
    "#     features['FEMININE'] = int(word.endswith('а'))\n",
    "#     features['MASCULINE'] = int(word.endswith('й'))\n",
    "\n",
    "#     # Check last 3 letters for plural nouns\n",
    "#     features['PLURAL'] = int(word.endswith('ие'))\n",
    "\n",
    "#     # Check for diminutive suffixes\n",
    "#     features['DIMINUTIVE'] = 1 if re.search(r'ок$|ек$|ик$|очк$', word) else 0\n",
    "\n",
    "#     # Check last 4 letters for reflexive verbs\n",
    "#     features['REFLEXIVE'] = 1 if re.search(r'ся$|сь$', word) else 0\n",
    "    \n",
    "#     # Check for punctuation\n",
    "#     features['has_punctuation'] = int(any(char in word for char in r'!?.,\"-')) \n",
    "    \n",
    "#     return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use K nearest neighbours to tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Calculate euclidean distance between training feature vector and test feature vector\n",
    "# def euclidean_distance(a, b):\n",
    "#     return (sum((a[x1] - b[x2])**2 for x1, x2 in zip(a, b)))**0.5\n",
    "\n",
    "# # Build training set\n",
    "# def build_training_set(sentences, get_features_func):\n",
    "#     X, y = [], []\n",
    "#     for sentence in sentences:\n",
    "#         for word, tag in sentence:\n",
    "#             X.append(get_features_func(word))\n",
    "#             y.append(tag)\n",
    "#     return X, y\n",
    "        \n",
    "# # kNN model, default k = 5\n",
    "# def predict(training_set, word, get_features_func, k=5):\n",
    "#     input_features = get_features_func(word)\n",
    "#     distances = defaultdict(float)\n",
    "#     X, y = build_training_set(training_set, get_features_func)\n",
    "\n",
    "#     # Compute distances between X_test and all training points\n",
    "#     dist = [(euclidean_distance(input_features, x_train), y) \n",
    "#             for (x_train, y) in zip(X, y)]\n",
    "    \n",
    "#     # Sort distances and take top k\n",
    "#     nearest = sorted(dist)[:k]\n",
    "\n",
    "#     # Count votes for each class\n",
    "#     votes = [y for _, y in nearest]\n",
    "#     prediction = max(set(votes), key=votes.count)\n",
    "    \n",
    "#     return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def lists_to_csv(list_a, list_b, filename):\n",
    "#     with open(filename, 'w', encoding='utf-8') as file:\n",
    "#         for a, b in zip(list_a, list_b):\n",
    "#             file.write(f'{a} {b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>iluminación</td>\n",
       "      <td>B-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>iluminación</td>\n",
       "      <td>B-neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>iluminación</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18153</th>\n",
       "      <td>iluminación</td>\n",
       "      <td>B-positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24403</th>\n",
       "      <td>iluminación</td>\n",
       "      <td>B-neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word       label\n",
       "349    iluminación  B-negative\n",
       "2101   iluminación   B-neutral\n",
       "2767   iluminación           O\n",
       "18153  iluminación  B-positive\n",
       "24403  iluminación   B-neutral"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rows = ES_train[ES_train['word'] == 'iluminación']\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Estuvimos', 'O'): 1.0,\n",
       " ('hace', 'O'): 1.0,\n",
       " ('poco', 'O'): 1.0,\n",
       " ('mi', 'O'): 1.0,\n",
       " ('pareja', 'O'): 1.0,\n",
       " ('y', 'O'): 0.9932104752667313,\n",
       " ('yo', 'O'): 0.972972972972973,\n",
       " ('comiendo', 'O'): 1.0,\n",
       " ('resultó', 'O'): 1.0,\n",
       " ('todo', 'O'): 0.9913793103448276,\n",
       " ('muy', 'O'): 0.9949748743718593,\n",
       " ('bien', 'O'): 0.9939759036144579,\n",
       " (',', 'O'): 0.9970041941282205,\n",
       " ('tanto', 'O'): 1.0,\n",
       " ('la', 'O'): 0.9792477302204928,\n",
       " ('comida', 'B-positive'): 0.7444933920704846,\n",
       " ('el', 'O'): 0.9907407407407407,\n",
       " ('vino', 'B-positive'): 0.15384615384615385,\n",
       " ('trato', 'B-positive'): 0.7333333333333333,\n",
       " ('decoración', 'B-positive'): 0.7777777777777778,\n",
       " ('…', 'O'): 0.9782608695652174,\n",
       " ('nos', 'O'): 1.0,\n",
       " ('gustó', 'O'): 1.0,\n",
       " ('mucho', 'O'): 0.9814814814814815,\n",
       " ('.', 'O'): 0.999384236453202,\n",
       " ('Por', 'O'): 1.0,\n",
       " ('poner', 'O'): 1.0,\n",
       " ('algún', 'O'): 1.0,\n",
       " ('pero', 'O'): 0.9896373056994818,\n",
       " ('quizá', 'O'): 1.0,\n",
       " ('jamón', 'B-negative'): 0.16666666666666666,\n",
       " ('no', 'O'): 0.9972972972972973,\n",
       " ('era', 'O'): 1.0,\n",
       " ('lo', 'O'): 1.0,\n",
       " ('\"', 'O'): 0.9069767441860465,\n",
       " ('ibérico', 'O'): 1.0,\n",
       " ('que', 'O'): 1.0,\n",
       " ('cabía', 'O'): 1.0,\n",
       " ('esperar', 'O'): 1.0,\n",
       " ('Bien', 'O'): 1.0,\n",
       " ('sabe', 'O'): 1.0,\n",
       " ('autor', 'O'): 1.0,\n",
       " ('del', 'O'): 0.9457364341085271,\n",
       " ('blog', 'O'): 1.0,\n",
       " (')', 'O'): 1.0,\n",
       " ('Comida', 'B-positive'): 0.8666666666666667,\n",
       " ('exquisita', 'O'): 1.0,\n",
       " ('Restaurante', 'B-positive'): 0.4444444444444444,\n",
       " ('diferente', 'O'): 1.0,\n",
       " ('creativo', 'O'): 1.0,\n",
       " ('agradable', 'O'): 1.0,\n",
       " ('Si', 'O'): 1.0,\n",
       " ('has', 'O'): 1.0,\n",
       " ('probado', 'O'): 1.0,\n",
       " ('sus', 'O'): 1.0,\n",
       " ('carnes', 'B-positive'): 0.6923076923076923,\n",
       " ('te', 'O'): 1.0,\n",
       " ('estas', 'O'): 1.0,\n",
       " ('perdiendo', 'O'): 1.0,\n",
       " ('algo', 'O'): 1.0,\n",
       " ('grande', 'O'): 1.0,\n",
       " ('!', 'O'): 1.0,\n",
       " ('En', 'O'): 0.975609756097561,\n",
       " ('resumen', 'O'): 1.0,\n",
       " ('bien-muy', 'O'): 1.0,\n",
       " ('servicio', 'B-positive'): 0.7176470588235294,\n",
       " ('correcto', 'O'): 1.0,\n",
       " ('profesional', 'O'): 1.0,\n",
       " ('02-12-', 'O'): 1.0,\n",
       " ('2012', 'O'): 1.0,\n",
       " ('elegimos', 'O'): 1.0,\n",
       " ('este', 'O'): 1.0,\n",
       " ('restaurante', 'B-negative'): 0.11842105263157894,\n",
       " ('por', 'O'): 0.996031746031746,\n",
       " ('los', 'O'): 0.988,\n",
       " ('comentarios', 'O'): 1.0,\n",
       " ('ha', 'O'): 1.0,\n",
       " ('sido', 'O'): 1.0,\n",
       " ('una', 'O'): 1.0,\n",
       " ('Grandisima', 'O'): 1.0,\n",
       " ('Decepción', 'O'): 1.0,\n",
       " ('Salimos', 'O'): 1.0,\n",
       " ('encantadas', 'O'): 1.0,\n",
       " ('restaurante', 'B-positive'): 0.3026315789473684,\n",
       " ('Ubicación', 'O'): 1.0,\n",
       " ('Comimos', 'O'): 0.9090909090909091,\n",
       " ('atendidos', 'O'): 1.0,\n",
       " ('Los', 'O'): 0.9666666666666667,\n",
       " ('servicios', 'B-negative'): 0.5,\n",
       " ('en', 'O'): 0.9940944881889764,\n",
       " ('estado', 'O'): 1.0,\n",
       " ('lamentable', 'O'): 1.0,\n",
       " ('manteles', 'B-negative'): 1.0,\n",
       " ('copas', 'B-negative'): 0.1,\n",
       " ('etc', 'O'): 1.0,\n",
       " ('<->', 'O'): 1.0,\n",
       " ('Gambas', 'B-negative'): 1.0,\n",
       " ('resecas', 'O'): 1.0,\n",
       " ('sabemos', 'O'): 1.0,\n",
       " ('llenaron', 'O'): 1.0,\n",
       " ('con', 'O'): 0.9331395348837209,\n",
       " ('trozos', 'O'): 1.0,\n",
       " ('de', 'O'): 0.9182561307901907,\n",
       " ('esa', 'O'): 1.0,\n",
       " ('sucedanio', 'B-neutral'): 1.0,\n",
       " ('en', 'I-neutral'): 0.001968503937007874,\n",
       " ('forma', 'I-neutral'): 0.2,\n",
       " ('de', 'I-neutral'): 0.004541326067211626,\n",
       " ('palo', 'I-neutral'): 0.5,\n",
       " ('-', 'I-neutral'): 0.03333333333333333,\n",
       " ('pero', 'I-neutral'): 0.0051813471502590676,\n",
       " ('con', 'I-neutral'): 0.011627906976744186,\n",
       " ('sabor', 'I-neutral'): 0.0625,\n",
       " ('a', 'I-neutral'): 0.0044943820224719105,\n",
       " ('gamba', 'I-neutral'): 0.3333333333333333,\n",
       " ('casi', 'O'): 1.0,\n",
       " ('caucho-imposible', 'O'): 1.0,\n",
       " ('comer', 'O'): 0.9777777777777777,\n",
       " ('supuesto', 'O'): 1.0,\n",
       " ('ya', 'O'): 1.0,\n",
       " ('estoy', 'O'): 1.0,\n",
       " ('dando', 'O'): 1.0,\n",
       " ('voces', 'O'): 1.0,\n",
       " ('para', 'O'): 0.9896373056994818,\n",
       " ('avisar', 'O'): 1.0,\n",
       " ('a', 'O'): 0.9640449438202248,\n",
       " ('todas', 'O'): 1.0,\n",
       " ('mis', 'O'): 1.0,\n",
       " ('relaciones', 'O'): 1.0,\n",
       " ('Borja', 'O'): 1.0,\n",
       " ('Alagarmos', 'O'): 1.0,\n",
       " ('cena', 'O'): 0.6842105263157895,\n",
       " ('hasta', 'O'): 1.0,\n",
       " ('las', 'O'): 0.9770992366412213,\n",
       " ('tres', 'O'): 0.8333333333333334,\n",
       " ('tomando', 'O'): 1.0,\n",
       " ('copas', 'O'): 0.8,\n",
       " ('Ofrecen', 'O'): 1.0,\n",
       " ('merluza', 'O'): 0.6,\n",
       " ('salsa', 'O'): 0.42857142857142855,\n",
       " ('El', 'O'): 0.9873417721518988,\n",
       " ('Minicuit', 'B-positive'): 1.0,\n",
       " ('igual', 'O'): 1.0,\n",
       " ('Me', 'O'): 1.0,\n",
       " ('invitaron', 'O'): 1.0,\n",
       " ('restaurante', 'O'): 0.5789473684210527,\n",
       " ('cual', 'O'): 1.0,\n",
       " ('desconocia', 'O'): 1.0,\n",
       " ('su', 'O'): 0.9770114942528736,\n",
       " ('existencia', 'O'): 1.0,\n",
       " ('De', 'O'): 0.9230769230769231,\n",
       " ('formas', 'O'): 1.0,\n",
       " ('decidimos', 'O'): 1.0,\n",
       " ('quedarnos', 'O'): 1.0,\n",
       " ('domingo', 'O'): 1.0,\n",
       " ('15', 'O'): 1.0,\n",
       " ('marzo', 'O'): 1.0,\n",
       " ('me', 'O'): 1.0,\n",
       " ('desplace', 'O'): 1.0,\n",
       " ('desde', 'O'): 1.0,\n",
       " ('galicia', 'O'): 1.0,\n",
       " ('unos', 'O'): 1.0,\n",
       " ('amigos', 'O'): 1.0,\n",
       " ('competir', 'O'): 1.0,\n",
       " ('maraton', 'O'): 1.0,\n",
       " ('bcn', 'O'): 1.0,\n",
       " ('Para', 'O'): 0.95,\n",
       " ('guiris', 'O'): 1.0,\n",
       " ('cominda', 'B-positive'): 1.0,\n",
       " ('impresionante', 'O'): 1.0,\n",
       " ('atencion', 'O'): 0.6428571428571429,\n",
       " ('aperitivo', 'O'): 0.75,\n",
       " ('es', 'O'): 0.9967213114754099,\n",
       " ('ni', 'O'): 1.0,\n",
       " ('normal', 'O'): 1.0,\n",
       " ('bueno', 'O'): 0.9821428571428571,\n",
       " ('un', 'O'): 1.0,\n",
       " ('recomendar', 'O'): 1.0,\n",
       " ('todos', 'O'): 0.967741935483871,\n",
       " ('Calidad', 'O'): 1.0,\n",
       " ('alimentos', 'B-positive'): 1.0,\n",
       " ('excelente', 'O'): 1.0,\n",
       " ('mesas', 'O'): 0.48,\n",
       " ('se', 'O'): 1.0,\n",
       " ('mostraron', 'O'): 1.0,\n",
       " ('atentos', 'O'): 0.9,\n",
       " ('momento', 'O'): 1.0,\n",
       " ('-', 'O'): 0.9333333333333333,\n",
       " ('10', 'O'): 1.0,\n",
       " ('Excelentes', 'O'): 1.0,\n",
       " ('istalaciones', 'B-positive'): 1.0,\n",
       " ('buena', 'O'): 1.0,\n",
       " ('atención', 'B-positive'): 0.6744186046511628,\n",
       " ('e', 'O'): 1.0,\n",
       " ('inmejorable', 'O'): 1.0,\n",
       " ('cocina', 'B-positive'): 0.36,\n",
       " ('digno', 'O'): 1.0,\n",
       " ('probar', 'O'): 1.0,\n",
       " ('Las', 'O'): 1.0,\n",
       " ('a', 'I-positive'): 0.02696629213483146,\n",
       " ('la', 'I-positive'): 0.01556420233463035,\n",
       " ('brasa', 'I-positive'): 0.5,\n",
       " ('son', 'O'): 1.0,\n",
       " ('superiores', 'O'): 1.0,\n",
       " ('Se', 'O'): 1.0,\n",
       " ('podria', 'O'): 1.0,\n",
       " ('mejorar', 'O'): 1.0,\n",
       " ('iluminación', 'B-negative'): 0.2,\n",
       " ('planta', 'O'): 1.0,\n",
       " ('superior', 'O'): 1.0,\n",
       " ('demasiada', 'O'): 1.0,\n",
       " ('luz', 'O'): 0.3333333333333333,\n",
       " ('plana', 'O'): 1.0,\n",
       " ('Tienen', 'O'): 1.0,\n",
       " ('carta', 'B-positive'): 0.3953488372093023,\n",
       " ('bastante', 'O'): 0.9705882352941176,\n",
       " ('extensa', 'O'): 1.0,\n",
       " ('donde', 'O'): 1.0,\n",
       " ('cada', 'O'): 1.0,\n",
       " ('plato', 'O'): 0.775,\n",
       " ('parece', 'O'): 1.0,\n",
       " ('mejor', 'O'): 1.0,\n",
       " ('anterior', 'O'): 1.0,\n",
       " ('Yo', 'O'): 1.0,\n",
       " ('recomiendo', 'O'): 1.0,\n",
       " ('Aunque', 'O'): 1.0,\n",
       " ('había', 'O'): 1.0,\n",
       " ('fumadores', 'O'): 1.0,\n",
       " ('mesa', 'O'): 0.9230769230769231,\n",
       " ('al', 'O'): 0.923728813559322,\n",
       " ('lado', 'O'): 1.0,\n",
       " ('sentimos', 'O'): 1.0,\n",
       " ('ninguna', 'O'): 1.0,\n",
       " ('molestia', 'O'): 1.0,\n",
       " ('durante', 'O'): 1.0,\n",
       " ('comida', 'O'): 0.14096916299559473,\n",
       " ('Hace', 'O'): 1.0,\n",
       " ('años', 'O'): 1.0,\n",
       " ('conozco', 'O'): 1.0,\n",
       " ('ponerle', 'O'): 1.0,\n",
       " ('pega', 'O'): 1.0,\n",
       " ('excesivo', 'O'): 1.0,\n",
       " ('ruido', 'B-negative'): 0.36363636363636365,\n",
       " ('mucha', 'O'): 1.0,\n",
       " ('gente', 'O'): 0.9473684210526315,\n",
       " ('sala', 'O'): 0.8571428571428571,\n",
       " ('resuena', 'O'): 1.0,\n",
       " ('podíamos', 'O'): 1.0,\n",
       " ('escucharnos', 'O'): 1.0,\n",
       " ('Rapidez', 'B-positive'): 0.6666666666666666,\n",
       " ('peticion', 'O'): 1.0,\n",
       " ('platos', 'O'): 0.589041095890411,\n",
       " ('solucitud', 'O'): 1.0,\n",
       " ('cuenta', 'O'): 0.9473684210526315,\n",
       " ('Lo', 'O'): 0.972972972972973,\n",
       " ('unico', 'O'): 1.0,\n",
       " ('puedo', 'O'): 1.0,\n",
       " ('decir', 'O'): 1.0,\n",
       " ('calidad', 'O'): 0.9470198675496688,\n",
       " ('comimos', 'O'): 1.0,\n",
       " ('fue', 'O'): 1.0,\n",
       " ('estupenda', 'O'): 1.0,\n",
       " ('sorprendio', 'O'): 1.0,\n",
       " ('cantidad', 'O'): 1.0,\n",
       " ('mas', 'O'): 1.0,\n",
       " ('escasa', 'O'): 1.0,\n",
       " ('(', 'O'): 1.0,\n",
       " ('peor', 'O'): 1.0,\n",
       " ('puede', 'O'): 1.0,\n",
       " ('pasar', 'O'): 1.0,\n",
       " ('gusta', 'O'): 1.0,\n",
       " ('opinar', 'O'): 1.0,\n",
       " ('pasado', 'O'): 1.0,\n",
       " ('tiempo', 'O'): 0.9230769230769231,\n",
       " ('cuando', 'O'): 1.0,\n",
       " ('asimilado', 'O'): 1.0,\n",
       " ('experiencia', 'O'): 0.6,\n",
       " ('A', 'O'): 1.0,\n",
       " ('veces', 'O'): 1.0,\n",
       " ('cruda', 'O'): 1.0,\n",
       " ('algunas', 'O'): 1.0,\n",
       " ('zonas', 'O'): 1.0,\n",
       " ('Servicio', 'B-positive'): 0.7428571428571429,\n",
       " ('La', 'O'): 0.9866666666666667,\n",
       " ('bebida', 'B-negative'): 0.5,\n",
       " ('dejado', 'O'): 1.0,\n",
       " ('desear', 'O'): 1.0,\n",
       " ('sentidos', 'O'): 1.0,\n",
       " ('poca', 'O'): 1.0,\n",
       " ('incluido', 'O'): 1.0,\n",
       " ('cervezas', 'O'): 1.0,\n",
       " ('hemos', 'O'): 1.0,\n",
       " ('tenido', 'O'): 1.0,\n",
       " ('pagar', 'O'): 1.0,\n",
       " ('extra', 'O'): 1.0,\n",
       " ('pequeño', 'O'): 1.0,\n",
       " ('porque', 'O'): 1.0,\n",
       " ('barato', 'O'): 1.0,\n",
       " ('poder', 'O'): 1.0,\n",
       " ('acabar', 'O'): 1.0,\n",
       " ('menú', 'O'): 0.5072463768115942,\n",
       " ('líquido', 'O'): 1.0,\n",
       " ('hacer', 'O'): 1.0,\n",
       " ('bajar', 'O'): 1.0,\n",
       " ('“', 'O'): 0.9666666666666667,\n",
       " ('rissoto', 'O'): 1.0,\n",
       " ('”', 'O'): 0.9714285714285714,\n",
       " ('hubiera', 'O'): 1.0,\n",
       " ('hecho', 'O'): 1.0,\n",
       " ('delicias', 'O'): 0.6666666666666666,\n",
       " ('presentador', 'O'): 1.0,\n",
       " ('Bricomanía', 'O'): 1.0,\n",
       " ('capítulo', 'O'): 1.0,\n",
       " ('explica', 'O'): 1.0,\n",
       " ('como', 'O'): 1.0,\n",
       " ('rebozar', 'O'): 1.0,\n",
       " ('paredes', 'O'): 1.0,\n",
       " ('viejas', 'O'): 1.0,\n",
       " ('precioso', 'O'): 1.0,\n",
       " ('moderno', 'O'): 1.0,\n",
       " ('acogedor', 'O'): 1.0,\n",
       " ('Como', 'O'): 1.0,\n",
       " ('soy', 'O'): 1.0,\n",
       " ('tierra', 'O'): 1.0,\n",
       " ('les', 'O'): 0.9523809523809523,\n",
       " ('dije', 'O'): 1.0,\n",
       " ('ir', 'O'): 1.0,\n",
       " ('calçotada', 'O'): 0.3076923076923077,\n",
       " ('probaran', 'O'): 1.0,\n",
       " ('Galicia', 'O'): 1.0,\n",
       " ('suele', 'O'): 1.0,\n",
       " ('JR', 'O'): 1.0,\n",
       " ('Buena', 'O'): 1.0,\n",
       " ('precio', 'O'): 0.9491525423728814,\n",
       " ('razonable', 'O'): 1.0,\n",
       " ('comida', 'B-neutral'): 0.039647577092511016,\n",
       " ('sin', 'O'): 0.9879518072289156,\n",
       " ('echar', 'O'): 1.0,\n",
       " ('cohetes', 'O'): 1.0,\n",
       " ('Enhorabuena', 'O'): 1.0,\n",
       " ('Bal', 'O'): 1.0,\n",
       " ('D', 'O'): 1.0,\n",
       " ('’', 'O'): 1.0,\n",
       " ('Onsera', 'O'): 1.0,\n",
       " ('conservar', 'O'): 1.0,\n",
       " ('estrella', 'O'): 1.0,\n",
       " ('Michelin', 'O'): 1.0,\n",
       " ('2010', 'O'): 1.0,\n",
       " ('lamento', 'O'): 1.0,\n",
       " ('hayan', 'O'): 1.0,\n",
       " ('quitado', 'O'): 1.0,\n",
       " ('Lillas', 'O'): 1.0,\n",
       " ('Pastia', 'O'): 1.0,\n",
       " ('Hacia', 'O'): 1.0,\n",
       " ('frio', 'B-negative'): 0.14285714285714285,\n",
       " ('Copio', 'O'): 1.0,\n",
       " ('escribí', 'O'): 1.0,\n",
       " ('unas', 'O'): 1.0,\n",
       " ('semanas', 'O'): 1.0,\n",
       " ('otra', 'O'): 1.0,\n",
       " ('página', 'O'): 1.0,\n",
       " ('web', 'O'): 0.8571428571428571,\n",
       " ('tras', 'O'): 0.6666666666666666,\n",
       " ('también', 'O'): 1.0,\n",
       " ('dolorasa', 'O'): 1.0,\n",
       " ('visita', 'O'): 1.0,\n",
       " ('sótano', 'B-negative'): 1.0,\n",
       " ('Fui', 'O'): 1.0,\n",
       " ('dado', 'O'): 1.0,\n",
       " ('impresión', 'O'): 1.0,\n",
       " ('tapas', 'O'): 0.75,\n",
       " ('excepcional', 'O'): 1.0,\n",
       " ('gusto', 'O'): 1.0,\n",
       " ('Rapido', 'O'): 1.0,\n",
       " ('cómodo', 'O'): 1.0,\n",
       " ('eficaz', 'O'): 1.0,\n",
       " ('sitio', 'B-neutral'): 0.046511627906976744,\n",
       " ('está', 'O'): 0.9666666666666667,\n",
       " ('mal', 'O'): 1.0,\n",
       " ('clasifican', 'O'): 1.0,\n",
       " ('típica', 'O'): 1.0,\n",
       " ('catalana', 'O'): 1.0,\n",
       " ('dieta', 'O'): 1.0,\n",
       " ('mediterránea', 'O'): 1.0,\n",
       " ('realidad', 'O'): 1.0,\n",
       " ('pizzería-grill', 'O'): 1.0,\n",
       " ('además', 'O'): 1.0,\n",
       " ('sirven', 'O'): 1.0,\n",
       " ('calçots', 'O'): 0.42857142857142855,\n",
       " ('escalibada', 'O'): 1.0,\n",
       " ('embutidos', 'O'): 1.0,\n",
       " ('No', 'O'): 0.9821428571428571,\n",
       " ('entiendo', 'O'): 1.0,\n",
       " ('reservaron', 'O'): 1.0,\n",
       " ('espacio', 'O'): 0.6,\n",
       " ('cerrado', 'O'): 1.0,\n",
       " ('salones', 'O'): 1.0,\n",
       " ('tienen', 'O'): 1.0,\n",
       " ('DecepcionanteCon', 'O'): 1.0,\n",
       " ('motivo', 'O'): 1.0,\n",
       " ('viaje', 'O'): 1.0,\n",
       " ('Zaragoza', 'O'): 1.0,\n",
       " ('cenamos', 'O'): 1.0,\n",
       " ('aconsejados', 'O'): 1.0,\n",
       " ('hotel', 'O'): 1.0,\n",
       " ('Colette', 'O'): 1.0,\n",
       " ('Muy', 'O'): 1.0,\n",
       " ('cocinados', 'O'): 1.0,\n",
       " ('18/03', 'O'): 1.0,\n",
       " ('/', 'O'): 1.0,\n",
       " ('2008', 'O'): 1.0,\n",
       " ('Buen', 'O'): 1.0,\n",
       " ('ambiente', 'B-positive'): 0.7333333333333333,\n",
       " ('buen', 'O'): 1.0,\n",
       " ('céntrico', 'O'): 1.0,\n",
       " ('buenos', 'O'): 1.0,\n",
       " ('precios', 'O'): 0.9259259259259259,\n",
       " ('bacalao', 'B-negative'): 0.2727272727272727,\n",
       " ('dorado', 'I-negative'): 1.0,\n",
       " ('revuelto', 'O'): 1.0,\n",
       " ('típico', 'O'): 1.0,\n",
       " ('bacalao', 'O'): 0.36363636363636365,\n",
       " ('deliciosa', 'O'): 1.0,\n",
       " ('atencion', 'B-positive'): 0.35714285714285715,\n",
       " ('demás', 'O'): 1.0,\n",
       " ('indico', 'O'): 1.0,\n",
       " ('inicio', 'O'): 1.0,\n",
       " ('trato', 'O'): 0.21666666666666667,\n",
       " ('Muchas', 'O'): 1.0,\n",
       " ('gracias', 'O'): 1.0,\n",
       " ('Mayúsculas', 'O'): 1.0,\n",
       " ('Gracias', 'O'): 1.0,\n",
       " ('tu', 'O'): 1.0,\n",
       " ('crónica', 'O'): 1.0,\n",
       " ('gran', 'O'): 1.0,\n",
       " ('ayuda', 'O'): 1.0,\n",
       " ('decidir', 'O'): 1.0,\n",
       " ('NO', 'O'): 1.0,\n",
       " ('celebrar', 'O'): 1.0,\n",
       " ('especial', 'O'): 1.0,\n",
       " ('dejaré', 'O'): 1.0,\n",
       " ('sitio', 'O'): 0.23255813953488372,\n",
       " ('críticos', 'O'): 1.0,\n",
       " ('gastronómicos', 'O'): 1.0,\n",
       " ('experimentados', 'O'): 1.0,\n",
       " ('Menú', 'O'): 0.8571428571428571,\n",
       " ('calçotada', 'B-positive'): 0.6153846153846154,\n",
       " ('recomendable', 'O'): 0.9761904761904762,\n",
       " ('relación', 'O'): 0.975,\n",
       " ('calidad-precio', 'O'): 1.0,\n",
       " ('Caótico', 'B-negative'): 1.0,\n",
       " ('Excelente', 'O'): 1.0,\n",
       " ('escaso', 'O'): 1.0,\n",
       " ('personal', 'B-negative'): 0.06896551724137931,\n",
       " ('esta', 'O'): 1.0,\n",
       " ('llenisimo', 'O'): 1.0,\n",
       " ('Vino', 'O'): 1.0,\n",
       " ('tinto', 'O'): 0.7142857142857143,\n",
       " ('O', 'O'): 1.0,\n",
       " ('Calatayud', 'O'): 0.5,\n",
       " ('Cinco', 'O'): 1.0,\n",
       " ('Platos', 'B-positive'): 0.5,\n",
       " ('cantidades', 'O'): 0.625,\n",
       " ('suficientes', 'O'): 1.0,\n",
       " ('llenarte.No', 'O'): 1.0,\n",
       " ('tardaban', 'O'): 1.0,\n",
       " ('nada', 'O'): 1.0,\n",
       " ('servirnos', 'O'): 1.0,\n",
       " ('siempre', 'O'): 1.0,\n",
       " ('tuviésemos', 'O'): 1.0,\n",
       " ('estaba', 'O'): 1.0,\n",
       " ('especialmente', 'O'): 1.0,\n",
       " ('huevos', 'B-positive'): 0.375,\n",
       " ('rotos', 'I-positive'): 1.0,\n",
       " ('cierto', 'O'): 1.0,\n",
       " ('explicaban', 'O'): 1.0,\n",
       " ('granja', 'O'): 1.0,\n",
       " ('masia', 'O'): 1.0,\n",
       " ('Estuve', 'O'): 1.0,\n",
       " ('viernes', 'O'): 1.0,\n",
       " ('noche', 'O'): 1.0,\n",
       " ('increible', 'O'): 1.0,\n",
       " ('camareros', 'O'): 0.14285714285714285,\n",
       " ('impecble', 'O'): 1.0,\n",
       " ('simpáticos', 'O'): 1.0,\n",
       " ('lograda', 'O'): 1.0,\n",
       " ('recomedable', 'O'): 1.0,\n",
       " ('vez', 'O'): 1.0,\n",
       " ('reserva', 'O'): 0.9285714285714286,\n",
       " ('ningún', 'O'): 1.0,\n",
       " ('problema', 'O'): 1.0,\n",
       " ('preguntándonos', 'O'): 1.0,\n",
       " ('si', 'O'): 1.0,\n",
       " ('orden', 'O'): 1.0,\n",
       " ('punto', 'O'): 1.0,\n",
       " ('segundo', 'O'): 0.9473684210526315,\n",
       " ('menos', 'O'): 1.0,\n",
       " ('cantiadad', 'O'): 1.0,\n",
       " ('Cenamos', 'O'): 1.0,\n",
       " ('señora', 'O'): 1.0,\n",
       " ('correctamente', 'O'): 1.0,\n",
       " ('ser', 'O'): 1.0,\n",
       " ('fuera', 'O'): 1.0,\n",
       " ('común', 'O'): 1.0,\n",
       " ('sorbetes', 'B-negative'): 1.0,\n",
       " ('fueron', 'O'): 1.0,\n",
       " ('mediocres', 'O'): 1.0,\n",
       " ('estaban', 'O'): 1.0,\n",
       " ('altura', 'O'): 1.0,\n",
       " ('pagado', 'O'): 1.0,\n",
       " ('aquí', 'O'): 1.0,\n",
       " ('sobre', 'O'): 0.9130434782608695,\n",
       " ('doce', 'O'): 1.0,\n",
       " ('cuarto', 'O'): 1.0,\n",
       " ('aparecen', 'O'): 1.0,\n",
       " ('Posesas', 'O'): 0.6666666666666666,\n",
       " ('empiezan', 'O'): 1.0,\n",
       " ('fotos', 'O'): 1.0,\n",
       " ('cliente', 'O'): 1.0,\n",
       " ('dar', 'O'): 1.0,\n",
       " ('vueltas', 'O'): 1.0,\n",
       " ('local', 'O'): 0.30952380952380953,\n",
       " ('Después', 'O'): 1.0,\n",
       " ('varios', 'O'): 1.0,\n",
       " ('restaurantes', 'O'): 0.8571428571428571,\n",
       " ('quedé', 'O'): 1.0,\n",
       " ('pelín', 'O'): 1.0,\n",
       " ('decepcionado', 'O'): 1.0,\n",
       " ('Comer', 'O'): 1.0,\n",
       " ('o', 'O'): 1.0,\n",
       " ('cenar', 'O'): 1.0,\n",
       " ('Barceloneta', 'O'): 1.0,\n",
       " ('garantía', 'O'): 1.0,\n",
       " ('ubicación', 'B-positive'): 1.0,\n",
       " ('EVITADLO', 'O'): 1.0,\n",
       " ('SI', 'O'): 1.0,\n",
       " ('PODÉIS', 'O'): 1.0,\n",
       " ('Un', 'O'): 0.9824561403508771,\n",
       " ('pienso', 'O'): 1.0,\n",
       " ('volver', 'O'): 1.0,\n",
       " ('Venga', 'O'): 1.0,\n",
       " ('tan', 'O'): 1.0,\n",
       " ('dificil', 'O'): 1.0,\n",
       " ('p', 'O'): 1.0,\n",
       " ('besugo', 'B-negative'): 0.25,\n",
       " ('salsa', 'I-negative'): 0.07142857142857142,\n",
       " ('solo', 'O'): 1.0,\n",
       " ('contenia', 'O'): 1.0,\n",
       " ('cabeza', 'O'): 1.0,\n",
       " ('pedí', 'O'): 1.0,\n",
       " ('foie', 'B-neutral'): 0.1,\n",
       " ('tostadas', 'I-neutral'): 0.5,\n",
       " ('más', 'O'): 1.0,\n",
       " ('correcta', 'O'): 1.0,\n",
       " ('pocas', 'O'): 1.0,\n",
       " ('tostadas', 'O'): 0.5,\n",
       " ('foie', 'O'): 0.6,\n",
       " ('Todo', 'O'): 1.0,\n",
       " ('perfecto', 'O'): 1.0,\n",
       " ('Recepcionistas', 'B-positive'): 1.0,\n",
       " ('Sabado', 'O'): 1.0,\n",
       " ('13h', 'O'): 1.0,\n",
       " ('sea', 'O'): 1.0,\n",
       " ('cocina', 'O'): 0.5,\n",
       " ('agobios', 'O'): 1.0,\n",
       " ('paella', 'B-negative'): 0.38461538461538464,\n",
       " ('simplemente', 'O'): 1.0,\n",
       " ('Sin', 'O'): 1.0,\n",
       " ('dudarlo', 'O'): 1.0,\n",
       " ('repetiremos', 'O'): 1.0,\n",
       " ('primeros', 'O'): 1.0,\n",
       " ('vayase', 'O'): 1.0,\n",
       " ('Palafox', 'O'): 1.0,\n",
       " ('tiene', 'O'): 1.0,\n",
       " ('Jésús', 'O'): 1.0,\n",
       " ('bigotes', 'O'): 1.0,\n",
       " ('perfección', 'O'): 1.0,\n",
       " ('Sitio', 'B-positive'): 1.0,\n",
       " ('recomendado', 'O'): 1.0,\n",
       " ('partir', 'O'): 1.0,\n",
       " ('ahi', 'O'): 1.0,\n",
       " ('caras', 'O'): 1.0,\n",
       " ('serias', 'O'): 1.0,\n",
       " ('humor', 'O'): 1.0,\n",
       " ('distraidas', 'O'): 1.0,\n",
       " ('camareros', 'B-negative'): 0.42857142857142855,\n",
       " ('...', 'O'): 1.0,\n",
       " ('hacían', 'O'): 1.0,\n",
       " ('otras', 'O'): 1.0,\n",
       " ('abalanzaban', 'O'): 1.0,\n",
       " ('liquidar', 'O'): 1.0,\n",
       " ('apetecibles', 'O'): 1.0,\n",
       " ('difícil', 'O'): 1.0,\n",
       " ('decidirse', 'O'): 1.0,\n",
       " ('local', 'B-positive'): 0.42857142857142855,\n",
       " ('aco', 'O'): 1.0,\n",
       " ('*', 'O'): 1.0,\n",
       " ('edor', 'O'): 1.0,\n",
       " ('instalado', 'O'): 1.0,\n",
       " ('Altamente', 'O'): 1.0,\n",
       " ('tener', 'O'): 1.0,\n",
       " ('solicité', 'O'): 1.0,\n",
       " ('tenía', 'O'): 1.0,\n",
       " ('incremento', 'O'): 1.0,\n",
       " ('siendo', 'O'): 1.0,\n",
       " ('exactamente', 'O'): 1.0,\n",
       " ('marcaba', 'O'): 1.0,\n",
       " ('menú', 'B-positive'): 0.4057971014492754,\n",
       " ('elegido', 'O'): 1.0,\n",
       " ('reservado', 'O'): 1.0,\n",
       " ('internet', 'O'): 1.0,\n",
       " ('bonito', 'O'): 0.8571428571428571,\n",
       " ('excesible', 'O'): 1.0,\n",
       " ('Pedimos', 'O'): 1.0,\n",
       " ('menu', 'B-positive'): 0.32142857142857145,\n",
       " ('de', 'I-positive'): 0.051771117166212535,\n",
       " ('calçots', 'I-positive'): 0.14285714285714285,\n",
       " ('valia', 'O'): 1.0,\n",
       " ('pena', 'O'): 1.0,\n",
       " ('Te', 'O'): 1.0,\n",
       " ('gustará', 'O'): 1.0,\n",
       " ('quieres', 'O'): 1.0,\n",
       " ('quedar', 'O'): 1.0,\n",
       " ('conseguiras', 'O'): 1.0,\n",
       " ('Fabuloso', 'O'): 1.0,\n",
       " ('comienzo', 'O'): 1.0,\n",
       " ('año', 'O'): 1.0,\n",
       " ('estancia', 'O'): 1.0,\n",
       " ('genial', 'O'): 1.0,\n",
       " ('recibido', 'O'): 1.0,\n",
       " ('anteriores', 'O'): 1.0,\n",
       " ('vinos', 'B-negative'): 0.23076923076923078,\n",
       " ('carísimos', 'O'): 1.0,\n",
       " ('Ha', 'O'): 1.0,\n",
       " ('tiempos', 'B-positive'): 0.16666666666666666,\n",
       " ('ambiente', 'B-neutral'): 0.08888888888888889,\n",
       " ('quizas', 'O'): 1.0,\n",
       " ('demasiado', 'O'): 1.0,\n",
       " ('iluminado', 'O'): 1.0,\n",
       " ('sabor', 'O'): 0.75,\n",
       " ('cualquier', 'O'): 1.0,\n",
       " ('caso', 'O'): 1.0,\n",
       " ('Ahora', 'O'): 1.0,\n",
       " ('merece', 'O'): 1.0,\n",
       " ('vistazo', 'O'): 1.0,\n",
       " ('críticas', 'O'): 1.0,\n",
       " ('expertos', 'O'): 1.0,\n",
       " ('culinarios', 'O'): 1.0,\n",
       " ('Gustos', 'O'): 1.0,\n",
       " ('sera', 'O'): 1.0,\n",
       " ('vean', 'O'): 1.0,\n",
       " ('sitios', 'O'): 0.7142857142857143,\n",
       " ('deseo', 'O'): 1.0,\n",
       " ('postres', 'B-negative'): 0.24,\n",
       " ('ausencia', 'O'): 1.0,\n",
       " ('chocolate', 'O'): 1.0,\n",
       " ('personalmente', 'O'): 1.0,\n",
       " ('imperdonable', 'O'): 1.0,\n",
       " ('eso', 'O'): 1.0,\n",
       " ('cuestión', 'O'): 1.0,\n",
       " ('personal', 'O'): 0.2413793103448276,\n",
       " ('magnífico', 'O'): 1.0,\n",
       " ('caracoles', 'O'): 1.0,\n",
       " ('carta', 'O'): 0.4418604651162791,\n",
       " ('paletilla', 'O'): 0.5,\n",
       " ('cordero', 'O'): 0.5,\n",
       " ('sugerencia', 'O'): 1.0,\n",
       " ('tampoco', 'O'): 1.0,\n",
       " ('pan', 'O'): 0.5,\n",
       " ('pedido', 'O'): 0.8571428571428571,\n",
       " ('..', 'O'): 1.0,\n",
       " ('fin', 'O'): 0.875,\n",
       " ('os', 'O'): 1.0,\n",
       " ('piseis', 'O'): 1.0,\n",
       " ('sitio', 'B-negative'): 0.09302325581395349,\n",
       " ('cobraran', 'O'): 1.0,\n",
       " ('entrar', 'O'): 1.0,\n",
       " ('Hoy', 'O'): 1.0,\n",
       " ('he', 'O'): 1.0,\n",
       " ('ido', 'O'): 1.0,\n",
       " ('mia', 'O'): 1.0,\n",
       " ('padres', 'O'): 1.0,\n",
       " ('salido', 'O'): 1.0,\n",
       " ('defraudado', 'O'): 1.0,\n",
       " ('pusieron', 'O'): 1.0,\n",
       " ('cobrarlo', 'O'): 1.0,\n",
       " ('Carpaccio', 'B-negative'): 1.0,\n",
       " ('de', 'I-negative'): 0.025431425976385105,\n",
       " ('bonito', 'I-negative'): 0.07142857142857142,\n",
       " (':', 'O'): 1.0,\n",
       " ('Seria', 'O'): 1.0,\n",
       " ('discuto', 'O'): 1.0,\n",
       " ('supo', 'O'): 1.0,\n",
       " ('totalmente', 'O'): 1.0,\n",
       " ('insipido', 'O'): 1.0,\n",
       " ('tambien', 'O'): 1.0,\n",
       " ('tenia', 'O'): 1.0,\n",
       " ('remolacha', 'B-negative'): 1.0,\n",
       " ('insipida', 'O'): 1.0,\n",
       " ('helado', 'O'): 1.0,\n",
       " ('creo', 'O'): 1.0,\n",
       " ('recordar', 'O'): 1.0,\n",
       " ('mango', 'O'): 0.5,\n",
       " ('algun', 'O'): 1.0,\n",
       " ('Nos', 'O'): 1.0,\n",
       " ('dejamos', 'O'): 1.0,\n",
       " ('guiar', 'O'): 1.0,\n",
       " ('ofrecieron', 'O'): 1.0,\n",
       " ('entrantes', 'O'): 0.7222222222222222,\n",
       " ('compartir', 'O'): 1.0,\n",
       " ('buñuelos', 'O'): 0.3333333333333333,\n",
       " ('alioli', 'O'): 1.0,\n",
       " ('membrillo', 'O'): 1.0,\n",
       " ('calamares', 'O'): 0.6666666666666666,\n",
       " ('plancha', 'O'): 0.6,\n",
       " ('He', 'O'): 1.0,\n",
       " ('varias', 'O'): 1.0,\n",
       " ('amable', 'O'): 1.0,\n",
       " ('cuanto', 'O'): 1.0,\n",
       " ('producto', 'B-positive'): 0.7692307692307693,\n",
       " ('sorpresas', 'O'): 1.0,\n",
       " ('plato', 'B-negative'): 0.2,\n",
       " ('jamon', 'I-negative'): 0.25,\n",
       " ('entrante', 'I-negative'): 0.125,\n",
       " ('25', 'O'): 1.0,\n",
       " ('euros', 'O'): 1.0,\n",
       " ('?', 'O'): 1.0,\n",
       " ('Mal', 'O'): 0.3333333333333333,\n",
       " ('Tampoco', 'O'): 1.0,\n",
       " ('avisaron', 'O'): 1.0,\n",
       " ('menu', 'O'): 0.6428571428571429,\n",
       " ('ese', 'O'): 1.0,\n",
       " ('dia', 'O'): 0.9411764705882353,\n",
       " ('través', 'O'): 1.0,\n",
       " ('nunca', 'O'): 1.0,\n",
       " ('han', 'O'): 0.9565217391304348,\n",
       " ('valor', 'O'): 1.0,\n",
       " ('seguro', 'O'): 1.0,\n",
       " ('Tardaron', 'O'): 1.0,\n",
       " ('media', 'O'): 1.0,\n",
       " ('hora', 'O'): 1.0,\n",
       " ('traer', 'O'): 1.0,\n",
       " ('pedidas', 'O'): 1.0,\n",
       " ('beber', 'O'): 1.0,\n",
       " ('vino', 'O'): 0.6153846153846154,\n",
       " ('recomienda', 'O'): 1.0,\n",
       " ('casa', 'O'): 0.9,\n",
       " ('Viña', 'O'): 1.0,\n",
       " ('Salceda', 'O'): 1.0,\n",
       " ('Reserva', 'O'): 1.0,\n",
       " ('C', 'O'): 1.0,\n",
       " ('sitio', 'B-positive'): 0.627906976744186,\n",
       " ('repetir', 'O'): 1.0,\n",
       " ('pizzas', 'B-positive'): 0.6666666666666666,\n",
       " ('horno', 'O'): 0.8333333333333334,\n",
       " ('leña', 'O'): 1.0,\n",
       " ('buenas', 'O'): 1.0,\n",
       " ('Compruebo', 'O'): 1.0,\n",
       " ('acertado', 'O'): 1.0,\n",
       " ('eres', 'O'): 1.0,\n",
       " ('Aragonés', 'O'): 1.0,\n",
       " ('entendido', 'O'): 1.0,\n",
       " ('fórmula', 'O'): 1.0,\n",
       " ('estuvo', 'O'): 1.0,\n",
       " ('cenando', 'O'): 1.0,\n",
       " ('riquísima', 'O'): 1.0,\n",
       " ('camareras', 'O'): 0.4,\n",
       " ('carne', 'O'): 0.2631578947368421,\n",
       " ('fallo', 'O'): 1.0,\n",
       " ('lleva', 'O'): 1.0,\n",
       " ('va', 'O'): 1.0,\n",
       " ('atraída', 'O'): 1.0,\n",
       " ('diseño', 'O'): 1.0,\n",
       " ('fama', 'O'): 1.0,\n",
       " ('cogiendo', 'O'): 1.0,\n",
       " ('Fuimos', 'O'): 1.0,\n",
       " ('semana', 'O'): 1.0,\n",
       " ('pasada', 'O'): 1.0,\n",
       " ('descubrimiento', 'O'): 1.0,\n",
       " ('aunque', 'O'): 1.0,\n",
       " ('nadie', 'O'): 1.0,\n",
       " ('pidió', 'O'): 1.0,\n",
       " ('pizza', 'B-positive'): 0.25,\n",
       " ('pinta', 'O'): 1.0,\n",
       " ('volveremos', 'O'): 1.0,\n",
       " ('probarlas', 'O'): 1.0,\n",
       " ('comida', 'B-negative'): 0.07488986784140969,\n",
       " ('elevado', 'O'): 1.0,\n",
       " ('desgustación', 'I-positive'): 1.0,\n",
       " ('alrededor', 'O'): 1.0,\n",
       " ('50', 'O'): 1.0,\n",
       " ('€', 'O'): 0.9629629629629629,\n",
       " ('persona', 'O'): 1.0,\n",
       " ('perfecta', 'O'): 1.0,\n",
       " ('considero', 'O'): 1.0,\n",
       " ('caro', 'O'): 1.0,\n",
       " ('Supero', 'O'): 1.0,\n",
       " ('nuestras', 'O'): 1.0,\n",
       " ('expectativas', 'O'): 1.0,\n",
       " ('iluminación', 'B-neutral'): 0.4,\n",
       " ('acogedora', 'O'): 1.0,\n",
       " ('Sinceramente', 'O'): 1.0,\n",
       " ('asegurar', 'O'): 1.0,\n",
       " ('volveré', 'O'): 0.875,\n",
       " ('Restaurante', 'B-neutral'): 0.3333333333333333,\n",
       " ('pasta', 'O'): 0.5,\n",
       " ('americana', 'O'): 1.0,\n",
       " ('regular', 'O'): 1.0,\n",
       " ('carne', 'B-negative'): 0.3684210526315789,\n",
       " ('mejorable', 'O'): 1.0,\n",
       " ('alterar', 'O'): 1.0,\n",
       " ('numero', 'O'): 1.0,\n",
       " ('piezas', 'O'): 1.0,\n",
       " ('Ayer', 'O'): 1.0,\n",
       " ('estuvimos', 'O'): 1.0,\n",
       " ('Aragonia', 'O'): 1.0,\n",
       " ('MUY', 'O'): 1.0,\n",
       " ('RECOMENDABLE', 'O'): 1.0,\n",
       " ('UNA', 'O'): 1.0,\n",
       " ('MARAVILLA', 'O'): 1.0,\n",
       " ('OS', 'O'): 1.0,\n",
       " ('LO', 'O'): 1.0,\n",
       " ('RECOMIENDO', 'O'): 1.0,\n",
       " ('Fantastico', 'O'): 1.0,\n",
       " ('luz', 'B-positive'): 0.3333333333333333,\n",
       " ('natural', 'I-positive'): 1.0,\n",
       " ('entra', 'O'): 1.0,\n",
       " ('vidriera', 'O'): 1.0,\n",
       " ('terraza', 'O'): 0.4166666666666667,\n",
       " ('añadida', 'O'): 1.0,\n",
       " ('da', 'O'): 1.0,\n",
       " ('sensación', 'O'): 1.0,\n",
       " ('bienestar', 'O'): 1.0,\n",
       " ('conseguir', 'O'): 1.0,\n",
       " ('otros', 'O'): 1.0,\n",
       " ('lugares', 'O'): 1.0,\n",
       " ('Pero', 'O'): 1.0,\n",
       " ('nuevo', 'O'): 1.0,\n",
       " ('escasos', 'O'): 1.0,\n",
       " ('especiales', 'O'): 1.0,\n",
       " ('<+>', 'O'): 1.0,\n",
       " ('extraordinaria', 'O'): 1.0,\n",
       " ('pescados', 'B-positive'): 0.3333333333333333,\n",
       " ('mariscos', 'B-positive'): 1.0,\n",
       " ('Repetiría', 'O'): 1.0,\n",
       " ('exquisito', 'O'): 1.0,\n",
       " ('Hola', 'O'): 1.0,\n",
       " ('tomates', 'O'): 1.0,\n",
       " ('Almería', 'O'): 1.0,\n",
       " ('Y', 'O'): 1.0,\n",
       " ('imaginais', 'O'): 1.0,\n",
       " ('esfuerzo', 'O'): 1.0,\n",
       " ('hacemos', 'O'): 1.0,\n",
       " ('ajustarlos', 'O'): 1.0,\n",
       " ('allí', 'O'): 1.0,\n",
       " ('cuesta', 'O'): 1.0,\n",
       " ('tela', 'O'): 1.0,\n",
       " ('pna', 'O'): 1.0,\n",
       " ('ver', 'O'): 1.0,\n",
       " ('nuestros', 'O'): 1.0,\n",
       " ('clientes', 'O'): 1.0,\n",
       " ('saliendo', 'O'): 1.0,\n",
       " ('felices', 'O'): 1.0,\n",
       " ('contentos', 'O'): 1.0,\n",
       " ('productos', 'O'): 0.2222222222222222,\n",
       " ('decorado', 'O'): 0.8,\n",
       " ('postre', 'B-positive'): 0.32142857142857145,\n",
       " ('probamos', 'O'): 1.0,\n",
       " ('setas', 'O'): 0.75,\n",
       " ('pedir', 'O'): 1.0,\n",
       " ('Repetiremos', 'O'): 1.0,\n",
       " ('turistico', 'O'): 1.0,\n",
       " ('Hay', 'O'): 1.0,\n",
       " ('vinos', 'B-positive'): 0.23076923076923078,\n",
       " ('jóvenes', 'O'): 1.0,\n",
       " ('Cariñena', 'O'): 1.0,\n",
       " ('2', 'O'): 1.0,\n",
       " ('botella', 'O'): 0.875,\n",
       " ('día', 'O'): 1.0,\n",
       " ('siguiente', 'O'): 1.0,\n",
       " ('fuimos', 'O'): 1.0,\n",
       " ('aparcan', 'O'): 1.0,\n",
       " ('coche', 'O'): 1.0,\n",
       " ('solomillo', 'B-positive'): 0.75,\n",
       " ('ternera', 'I-positive'): 0.125,\n",
       " ('carne', 'B-positive'): 0.3684210526315789,\n",
       " ('justo', 'O'): 1.0,\n",
       " ('pedimos', 'O'): 1.0,\n",
       " ('Celebramos', 'O'): 1.0,\n",
       " ('cumpleaños', 'O'): 1.0,\n",
       " ('Trato', 'B-negative'): 0.125,\n",
       " ('pobre', 'O'): 1.0,\n",
       " ('platos', 'B-negative'): 0.136986301369863,\n",
       " ('sentido', 'O'): 1.0,\n",
       " ('local', 'B-negative'): 0.14285714285714285,\n",
       " ('ruido', 'O'): 0.45454545454545453,\n",
       " ('Cocina', 'B-positive'): 0.6666666666666666,\n",
       " ('platos', 'B-positive'): 0.2465753424657534,\n",
       " ('estan', 'O'): 1.0,\n",
       " ('exquisutos', 'O'): 1.0,\n",
       " ('12', 'O'): 1.0,\n",
       " ('compañeros', 'O'): 1.0,\n",
       " ('trabajo', 'O'): 1.0,\n",
       " ('cogimos', 'O'): 1.0,\n",
       " ('20', 'O'): 1.0,\n",
       " ('Absolutamente', 'O'): 1.0,\n",
       " ('dieron', 'O'): 1.0,\n",
       " ('opción', 'O'): 1.0,\n",
       " ('calsots', 'B-positive'): 0.5,\n",
       " ('detalle', 'O'): 0.8571428571428571,\n",
       " ('agradece', 'O'): 1.0,\n",
       " ('suficiente', 'O'): 1.0,\n",
       " ('aprobar', 'O'): 1.0,\n",
       " ('servicio', 'O'): 0.08235294117647059,\n",
       " ('Magnífica', 'O'): 1.0,\n",
       " ('atención', 'O'): 0.18604651162790697,\n",
       " ('Ana', 'B-positive'): 0.75,\n",
       " ('hoy', 'O'): 1.0,\n",
       " ('guinda', 'O'): 1.0,\n",
       " ('carajillo', 'O'): 1.0,\n",
       " ('ron', 'O'): 1.0,\n",
       " ('debe', 'O'): 1.0,\n",
       " ('obra', 'O'): 1.0,\n",
       " ('camarera', 'B-positive'): 0.5,\n",
       " ('Extremadamente', 'O'): 1.0,\n",
       " ('juntas', 'O'): 1.0,\n",
       " ('mesas', 'B-negative'): 0.32,\n",
       " ('alguno', 'O'): 1.0,\n",
       " ('Seguro', 'O'): 1.0,\n",
       " ('quedas', 'O'): 1.0,\n",
       " ('hambre', 'O'): 1.0,\n",
       " ('claidad', 'O'): 1.0,\n",
       " ('Calidad-precio', 'B-positive'): 1.0,\n",
       " ('hay', 'O'): 0.9705882352941176,\n",
       " ('negativo', 'O'): 1.0,\n",
       " ('exterior', 'O'): 1.0,\n",
       " ('luminoso', 'O'): 1.0,\n",
       " ('accesos', 'O'): 1.0,\n",
       " ('faciles', 'O'): 1.0,\n",
       " ('extraña', 'O'): 1.0,\n",
       " ('comentario', 'O'): 1.0,\n",
       " ('Marta', 'O'): 1.0,\n",
       " ('propietario', 'B-negative'): 0.5,\n",
       " ('debería', 'O'): 1.0,\n",
       " ('dedicarse', 'O'): 1.0,\n",
       " ('menesteres', 'O'): 1.0,\n",
       " ('interesante', 'O'): 1.0,\n",
       " ('genero', 'O'): 0.5,\n",
       " ('empezar', 'O'): 1.0,\n",
       " ('puerta', 'B-negative'): 0.6666666666666666,\n",
       " ('almacen', 'O'): 1.0,\n",
       " ('llamas', 'O'): 1.0,\n",
       " ('portero', 'O'): 1.0,\n",
       " ('automatico', 'O'): 1.0,\n",
       " ('abren', 'O'): 1.0,\n",
       " ('esto', 'O'): 1.0,\n",
       " ('direis', 'O'): 1.0,\n",
       " ('secundario', 'O'): 1.0,\n",
       " ('Tanto', 'O'): 1.0,\n",
       " ('historia', 'O'): 1.0,\n",
       " ('bodega', 'B-positive'): 0.8333333333333334,\n",
       " ('Unos', 'O'): 1.0,\n",
       " ('aperitivos', 'O'): 1.0,\n",
       " ('ofrecidos', 'O'): 1.0,\n",
       " ('cóctel', 'O'): 1.0,\n",
       " ('martini', 'O'): 1.0,\n",
       " ('blanco', 'O'): 0.7142857142857143,\n",
       " ('enfriado', 'O'): 1.0,\n",
       " ('nitrógeno', 'O'): 1.0,\n",
       " ('pastilla', 'O'): 1.0,\n",
       " ('mi-cuit', 'O'): 1.0,\n",
       " ('envuelto', 'O'): 1.0,\n",
       " ('frambuesa', 'O'): 1.0,\n",
       " ('sidral', 'O'): 1.0,\n",
       " ('algodón', 'O'): 1.0,\n",
       " ('coco', 'O'): 1.0,\n",
       " ('entndemos', 'O'): 1.0,\n",
       " ('qué', 'O'): 1.0,\n",
       " ('hicieron', 'O'): 1.0,\n",
       " ('escoger', 'O'): 1.0,\n",
       " ('entre', 'O'): 1.0,\n",
       " ('primer', 'O'): 1.0,\n",
       " ('turno', 'O'): 1.0,\n",
       " ('opinión', 'O'): 1.0,\n",
       " ('maitre', 'B-negative'): 0.4,\n",
       " ('tomó', 'O'): 1.0,\n",
       " ('nota', 'O'): 1.0,\n",
       " ('profesionalidad', 'O'): 1.0,\n",
       " ('rozando', 'O'): 1.0,\n",
       " ('mala', 'O'): 1.0,\n",
       " ('educación', 'O'): 0.8333333333333334,\n",
       " ('Increible', 'O'): 1.0,\n",
       " ('increíble', 'O'): 1.0,\n",
       " ('personal', 'B-positive'): 0.6206896551724138,\n",
       " ('magnifico', 'O'): 1.0,\n",
       " ('Creo', 'O'): 1.0,\n",
       " ('espaciaran', 'O'): 1.0,\n",
       " ('cambiaran', 'O'): 1.0,\n",
       " ('iluminación', 'O'): 0.2,\n",
       " ('calido', 'O'): 1.0,\n",
       " ...}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4_emission_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES Test (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spanish Test\n",
    "# es_list = ES_dev_in['word'].to_list()\n",
    "# es_prediction = []\n",
    "# for word in es_list:\n",
    "#     es_prediction.append(predict(es_sentences, word, get_features_es))\n",
    "# lists_to_csv(es_list, es_prediction, \"./ES/dev.p4.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU Test (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Russian Test\n",
    "# ru_list = RU_dev_in['word'].to_list()\n",
    "# ru_prediction = []\n",
    "# for word in ru_list:\n",
    "#     ru_prediction.append(predict(ru_sentences, word, get_features_ru))\n",
    "# lists_to_csv(ru_list, ru_prediction, \"./RU/dev.p4.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estuvimos</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hace</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poco</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pareja</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33028</th>\n",
       "      <td>con</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33029</th>\n",
       "      <td>la</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33030</th>\n",
       "      <td>carta</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33031</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33032</th>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33033 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word label\n",
       "0      Estuvimos     O\n",
       "1           hace     O\n",
       "2           poco     O\n",
       "3             mi     O\n",
       "4         pareja     O\n",
       "...          ...   ...\n",
       "33028        con     O\n",
       "33029         la     O\n",
       "33030      carta     O\n",
       "33031          .     O\n",
       "33032       null  null\n",
       "\n",
       "[33033 rows x 2 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimate_emission_parameters_word(train):\n",
    "    word_count = {}\n",
    "    emission_count = {}\n",
    "    label_count = {}\n",
    "\n",
    "    for row in range(train.shape[0]):\n",
    "        word = train.iloc[row, 0]\n",
    "        label = train.iloc[row, 1]\n",
    "        if (word not in word_count) and (word != \"null\"):\n",
    "            word_count[word] = 1\n",
    "        elif ( word != \"null\" ):\n",
    "            word_count[word] += 1\n",
    "        \n",
    "        if (label not in label_count) and (label != \"null\"):\n",
    "            label_count[label] = 1\n",
    "        elif (label != \"null\"):\n",
    "            label_count[label] += 1\n",
    "\n",
    "        if ((word, label) not in emission_count) and (word != \"null\"):\n",
    "            emission_count[(word, label)] = 1\n",
    "        elif (word != \"null\"):\n",
    "            emission_count[(word, label)] += 1\n",
    "\n",
    "    emission_parameter = {}\n",
    "    for word, label in emission_count:\n",
    "        emission_parameter[(word, label)] = emission_count[(word, label)] / word_count[word]\n",
    "\n",
    "    return emission_parameter, emission_count, label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgraded_estimate_emission_parameters_word(test_set, emission_parameter, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 0\n",
    "    special_word = '#UNK#'\n",
    "    test_copy = test_set.copy()\n",
    "    #train word list is all the first argument in keys of emission counts\n",
    "    train_word_list= [word for word, label in emission_counts.keys()]\n",
    "    for i in range(len(test_copy[\"word\"])):\n",
    "        word = test_copy[\"word\"][i]\n",
    "        if word not in train_word_list:\n",
    "            test_copy[\"word\"][i] = special_word\n",
    "            k+=1\n",
    "\n",
    "    for word in test_copy[\"word\"]:\n",
    "        for label in label_counts:\n",
    "            if (word == special_word) and ((word, label) not in emission_parameter):\n",
    "                emission_parameter[(word, label)] = (k*(label_counts.get(label)/sum(label_counts.values())))/k\n",
    "                # print((word, label), emission_parameter.get((word, label)))\n",
    "                      \n",
    "    return emission_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_ES_emit, q4_ES_emitcount, q4_ES_label_count = train_estimate_emission_parameters_word(ES_train)\n",
    "q4_ES_emit_fin = upgraded_estimate_emission_parameters_word(ES_dev_in, q4_ES_emit, q4_ES_emitcount, q4_ES_label_count)\n",
    "\n",
    "q4_RU_emit, q4_RU_emitcount, q4_RU_label_count = train_estimate_emission_parameters_word(RU_train)\n",
    "q4_RU_emit_fin = upgraded_estimate_emission_parameters_word(RU_dev_in, q4_RU_emit, q4_RU_emitcount, q4_RU_label_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_4404\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n"
     ]
    }
   ],
   "source": [
    "ES_pred_q4 = predict(q4_ES_emit_fin, ES_transit, ES_seq_list)\n",
    "RU_pred_q4 = predict(q4_RU_emit_fin, RU_transit, RU_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_proc_q4 = processing_pred(ES_pred_q4)\n",
    "RU_proc_q4 = processing_pred(RU_pred_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q4 = consolidate(ES_proc_q4, ES_seq_list)\n",
    "RU_pred_q4 = consolidate(RU_proc_q4, RU_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q4.to_csv(\"./ES/dev.p4.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_q4.to_csv(\"./RU/dev.p4.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 212\n",
      "\n",
      "#Correct Entity : 131\n",
      "Entity  precision: 0.6179\n",
      "Entity  recall: 0.5721\n",
      "Entity  F: 0.5941\n",
      "\n",
      "#Correct Sentiment : 98\n",
      "Sentiment  precision: 0.4623\n",
      "Sentiment  recall: 0.4279\n",
      "Sentiment  F: 0.4444\n",
      "\n",
      "RU result is:\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 276\n",
      "\n",
      "#Correct Entity : 181\n",
      "Entity  precision: 0.6558\n",
      "Entity  recall: 0.4653\n",
      "Entity  F: 0.5444\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.4674\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.3880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command4 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p4.out']\n",
    "result4 = subprocess.run(command4, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result4.stdout.decode())\n",
    "\n",
    "command4 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p4.out']\n",
    "result4 = subprocess.run(command4, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result4.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 4057\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "RU result is:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "\n",
    "# command3 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'./ES/dev.p4.out']\n",
    "# result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "# print(\"ES result is:\")\n",
    "# print(result3.stdout.decode())\n",
    "\n",
    "# command3 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'./RU/dev.p4.out']\n",
    "# result3 = subprocess.run(command3, stdout=subprocess.PIPE)\n",
    "# print(\"RU result is:\")\n",
    "# print(result3.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./ES/test.in', 'r', encoding = 'utf8') as f:\n",
    "#     r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "# ES_test_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "# ES_test_in = pd.DataFrame(data = ES_test_in_tup, columns = [\"word\"])\n",
    "# es_test_list = ES_test_in['word'].to_list()\n",
    "\n",
    "# with open('./RU/test.in', 'r', encoding = 'utf8') as f:\n",
    "#     r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "# RU_test_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "# RU_test_in = pd.DataFrame(data = RU_test_in_tup, columns = [\"word\"])\n",
    "# ru_test_list = RU_test_in['word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES Test (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spanish Test\n",
    "# es_test_prediction = []\n",
    "# for word in es_test_list:\n",
    "#     es_test_prediction.append(predict(es_sentences, word, get_features_es))\n",
    "# lists_to_csv(es_test_list, es_test_prediction, \"./ES/test.p4.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU Test (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Russian Test\n",
    "# ru_test_prediction = []\n",
    "# for word in ru_test_list:\n",
    "#     ru_test_prediction.append(predict(ru_sentences, word, get_features_ru))\n",
    "# lists_to_csv(ru_test_list, ru_test_prediction, \"./RU/test.p4.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
