{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group THE BOYS 1D ML project\n",
    "1. Loy Pek Yong 1004475\n",
    "2. Lim Yongqing 1005955\n",
    "\n",
    "\n",
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries + reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "\n",
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    r1 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r1]\n",
    "\n",
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r2 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r2]\n",
    "\n",
    "with open('./ES/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r3 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "ES_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r3]\n",
    "\n",
    "\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    r4 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_train_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r4]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    r5 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_in_tup = [ lines if lines != \"\" else \"null\" for lines in r5]\n",
    "\n",
    "with open('./RU/dev.out', 'r', encoding = 'utf8') as f:\n",
    "    r6 = [lines.rstrip(\"\\n\") for lines in f.readlines()]\n",
    "RU_dev_out_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else (\"null\", \"null\") for lines in r6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_train = pd.DataFrame(data = ES_train_tup, columns = [\"word\", \"label\"])\n",
    "ES_dev_in = pd.DataFrame(data = ES_dev_in_tup, columns = [\"word\"])\n",
    "ES_dev_out = pd.DataFrame(data = ES_dev_out_tup, columns = [\"word\", \"label\"])\n",
    "\n",
    "RU_train = pd.DataFrame(data = RU_train_tup, columns = [\"word\", \"label\"])\n",
    "RU_dev_in = pd.DataFrame(data = RU_dev_in_tup, columns = [\"word\"])\n",
    "RU_dev_out = pd.DataFrame(data = RU_dev_out_tup, columns = [\"word\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimate_emission_parameters(train):\n",
    "    # word_column = train[\"word\"]\n",
    "    # label_column = train[\"label\"]\n",
    "\n",
    "    label_count = {}\n",
    "    emission_count = {}\n",
    "\n",
    "    for row in range(train.shape[0]):\n",
    "        word = train.iloc[row, 0]\n",
    "        label = train.iloc[row, 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "\n",
    "        if (word, label) not in emission_count:\n",
    "            emission_count[(word, label)] = 1\n",
    "        else:\n",
    "            emission_count[(word, label)] += 1\n",
    "\n",
    "    emission_parameter = {}\n",
    "    for word, label in emission_count:\n",
    "        emission_parameter[(word, label)] = emission_count[(word, label)] / label_count[label]\n",
    "\n",
    "    return emission_parameter, emission_count, label_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that returns the emission parameters with input set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 0\n",
    "    special_word = '#UNK#'\n",
    "    test_copy = test_set.copy()\n",
    "    #train word list is all the first argument in keys of emission counts\n",
    "    train_word_list= [word for word, label in emission_counts.keys()]\n",
    "    for i in range(len(test_copy[\"word\"])):\n",
    "        word = test_copy[\"word\"][i]\n",
    "        if word not in train_word_list:\n",
    "            test_copy[\"word\"][i] = special_word\n",
    "            k+=1\n",
    "\n",
    "    for word in test_copy[\"word\"]:\n",
    "        for label in label_counts:\n",
    "            if word == special_word:\n",
    "                emission_parameter[(word, label)] = k / (label_counts[label] + k)\n",
    "            elif ( (word, label) in emission_parameter ):\n",
    "                emission_parameter[(word, label)] = emission_counts[(word, label)] / (label_counts[label] + k)\n",
    "                \n",
    "    # print(emission_parameter)        \n",
    "    return emission_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 of q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(training_set, test_set):\n",
    "    emission_parameter, emission_count, label_count = train_estimate_emission_parameters(training_set)\n",
    "    emission_parameter = upgraded_estimate_emission_parameters(test_set, emission_parameter, emission_count, label_count)\n",
    "    test_set[\"label\"] = \"\"\n",
    "\n",
    "    #label equals to argmax of emission parameter\n",
    "    for row in range(len(test_set)):\n",
    "        word = test_set.iloc[row][\"word\"]\n",
    "        max_val = 0\n",
    "        max_label = \"\"\n",
    "        train_word_list = [word for word, label in emission_count.keys()]\n",
    "        \n",
    "        for label in label_count:\n",
    "            if (word, label) in emission_parameter:\n",
    "                if emission_parameter[(word, label)] > max_val:\n",
    "                    max_val = emission_parameter[(word, label)]\n",
    "                    max_label = label\n",
    "                    \n",
    "            else:\n",
    "                if word not in train_word_list:\n",
    "                    # get max label from word #UNK#\n",
    "                    if (\"#UNK#\", label) in emission_parameter:\n",
    "                        if emission_parameter[(\"#UNK#\", label)] > max_val:\n",
    "                            max_val = emission_parameter[(\"#UNK#\", label)]\n",
    "                            max_label = label\n",
    "        test_set.loc[row, \"label\"] = max_label\n",
    "                    \n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting + saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_dev_p1_out = sentiment_analysis(ES_train, ES_dev_in)\n",
    "ES_dev_p1_out_r = ES_dev_p1_out.replace('null', pd.NA)\n",
    "ES_dev_p1_out_r.to_csv(\"./ES/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "    \n",
    "RU_dev_p1_out = sentiment_analysis(RU_train, RU_dev_in)\n",
    "RU_dev_p1_out_r = RU_dev_p1_out.replace('null', pd.NA)\n",
    "RU_dev_p1_out_r.to_csv(\"./RU/dev.p1.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\r\n",
      "#Entity in gold data: 229\r\n",
      "#Entity in prediction: 1144\r\n",
      "\r\n",
      "#Correct Entity : 181\r\n",
      "Entity  precision: 0.1582\r\n",
      "Entity  recall: 0.7904\r\n",
      "Entity  F: 0.2637\r\n",
      "\r\n",
      "#Correct Sentiment : 120\r\n",
      "Sentiment  precision: 0.1049\r\n",
      "Sentiment  recall: 0.5240\r\n",
      "Sentiment  F: 0.1748\r\n",
      "\n",
      "RU result is:\n",
      "\r\n",
      "#Entity in gold data: 389\r\n",
      "#Entity in prediction: 1467\r\n",
      "\r\n",
      "#Correct Entity : 288\r\n",
      "Entity  precision: 0.1963\r\n",
      "Entity  recall: 0.7404\r\n",
      "Entity  F: 0.3103\r\n",
      "\r\n",
      "#Correct Sentiment : 174\r\n",
      "Sentiment  precision: 0.1186\r\n",
      "Sentiment  recall: 0.4473\r\n",
      "Sentiment  F: 0.1875\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result.stdout.decode())\n",
    "\n",
    "command = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p1.out']\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "EStrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "EStrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in EStrain ]\n",
    "\n",
    "with open('./RU/train', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUtrain = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUtrain_tup = [ (lines.rsplit(\" \", 1)[0], lines.rsplit(\" \", 1)[1]) if lines != \"\" else \"STOP\" for lines in RUtrain ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reference transition dictionary (all transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_dict = {\n",
    "    (\"O\", \"O\"): 0,\n",
    "    (\"O\", \"B-positive\"):0,\n",
    "    (\"O\", \"B-negative\"):0,\n",
    "    (\"O\", \"B-neutral\"):0,\n",
    "    (\"O\", \"I-positive\"):0,\n",
    "    (\"O\", \"I-negative\"):0,\n",
    "    (\"O\", \"I-neutral\"):0,\n",
    "    (\"O\", \"START\"):0,\n",
    "    (\"O\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-positive\", \"O\"):0,\n",
    "    (\"B-positive\", \"B-positive\"):0,\n",
    "    (\"B-positive\", \"B-negative\"):0,\n",
    "    (\"B-positive\", \"B-neutral\"):0,\n",
    "    (\"B-positive\", \"I-positive\"):0,\n",
    "    (\"B-positive\", \"I-negative\"):0,\n",
    "    (\"B-positive\", \"I-neutral\"):0,\n",
    "    (\"B-positive\", \"START\"):0,\n",
    "    (\"B-positive\", \"STOP\"):0,\n",
    "    \n",
    "    \n",
    "    (\"B-negative\", \"O\"):0,\n",
    "    (\"B-negative\", \"B-positive\"):0,\n",
    "    (\"B-negative\", \"B-negative\"):0,\n",
    "    (\"B-negative\", \"B-neutral\"):0,\n",
    "    (\"B-negative\", \"I-positive\"):0,\n",
    "    (\"B-negative\", \"I-negative\"):0,\n",
    "    (\"B-negative\", \"I-neutral\"):0,\n",
    "    (\"B-negative\", \"START\"):0,\n",
    "    (\"B-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"B-neutral\", \"O\"):0,\n",
    "    (\"B-neutral\", \"B-positive\"):0,\n",
    "    (\"B-neutral\", \"B-negative\"):0,\n",
    "    (\"B-neutral\", \"B-neutral\"):0,\n",
    "    (\"B-neutral\", \"I-positive\"):0,\n",
    "    (\"B-neutral\", \"I-negative\"):0,\n",
    "    (\"B-neutral\", \"I-neutral\"):0,\n",
    "    (\"B-neutral\", \"START\"):0,\n",
    "    (\"B-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-positive\", \"O\"):0,\n",
    "    (\"I-positive\", \"B-positive\"):0,\n",
    "    (\"I-positive\", \"B-negative\"):0,\n",
    "    (\"I-positive\", \"B-neutral\"):0,\n",
    "    (\"I-positive\", \"I-positive\"):0,\n",
    "    (\"I-positive\", \"I-negative\"):0,\n",
    "    (\"I-positive\", \"I-neutral\"):0,\n",
    "    (\"I-positive\", \"START\"):0,\n",
    "    (\"I-positive\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-negative\", \"O\"):0,\n",
    "    (\"I-negative\", \"B-positive\"):0,\n",
    "    (\"I-negative\", \"B-negative\"):0,\n",
    "    (\"I-negative\", \"B-neutral\"):0,\n",
    "    (\"I-negative\", \"I-positive\"):0,\n",
    "    (\"I-negative\", \"I-negative\"):0,\n",
    "    (\"I-negative\", \"I-neutral\"):0,\n",
    "    (\"I-negative\", \"START\"):0,\n",
    "    (\"I-negative\", \"STOP\"):0,\n",
    "    \n",
    "    (\"I-neutral\", \"O\"):0,\n",
    "    (\"I-neutral\", \"B-positive\"):0,\n",
    "    (\"I-neutral\", \"B-negative\"):0,\n",
    "    (\"I-neutral\", \"B-neutral\"):0,\n",
    "    (\"I-neutral\", \"I-positive\"):0,\n",
    "    (\"I-neutral\", \"I-negative\"):0,\n",
    "    (\"I-neutral\", \"I-neutral\"):0,\n",
    "    (\"I-neutral\", \"START\"):0,\n",
    "    (\"I-neutral\", \"STOP\"):0,\n",
    "    \n",
    "    (\"START\", \"O\"):0,\n",
    "    (\"START\", \"B-positive\"):0,\n",
    "    (\"START\", \"B-negative\"):0,\n",
    "    (\"START\", \"B-neutral\"):0,\n",
    "    (\"START\", \"I-positive\"):0,\n",
    "    (\"START\", \"I-negative\"):0,\n",
    "    (\"START\", \"I-neutral\"):0,\n",
    "    (\"START\", \"START\"):0,\n",
    "    (\"START\", \"STOP\"):0,\n",
    "    \n",
    "    (\"STOP\", \"O\"):0,\n",
    "    (\"STOP\", \"B-positive\"):0,\n",
    "    (\"STOP\", \"B-negative\"):0,\n",
    "    (\"STOP\", \"B-neutral\"):0,\n",
    "    (\"STOP\", \"I-positive\"):0,\n",
    "    (\"STOP\", \"I-negative\"):0,\n",
    "    (\"STOP\", \"I-neutral\"):0,\n",
    "    (\"STOP\", \"START\"):0,\n",
    "    (\"STOP\", \"STOP\"):0,\n",
    "               \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate number of u (init state) to v (next state) transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assumptions made: \n",
    "\n",
    "1. Everything is in tuple pairs except the word \"STOP\" which signifies the end of a review\n",
    "2. Transition probabilities from \"STOP\" to any other states are 0 but still included in the dictionary as required by question\n",
    "\n",
    "\n",
    "\n",
    "Variables:\n",
    "1. u --> initial state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "2. v --> next state (e.g \"O\", \"B-neutral\", \"I-positive\" etc.)\n",
    "3. hasbreak --> signifies the end of a review (boolean)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_transition(data, transit_dict):\n",
    "    updated_dict = transit_dict.copy()\n",
    "    hasbreak = True\n",
    "    for word in data:\n",
    "        if ( hasbreak == True ):\n",
    "            u = \"START\"\n",
    "            \n",
    "        if ( word != \"STOP\" and (hasbreak == True) ):\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = False\n",
    "            \n",
    "        elif ( word != \"STOP\" and (hasbreak == False) ):\n",
    "            u = v\n",
    "            v = word[1]\n",
    "            updated_dict[(u, v)] += 1\n",
    "        \n",
    "        elif ( word == \"STOP\" ):\n",
    "            u = v\n",
    "            v = \"STOP\"\n",
    "            updated_dict[(u, v)] += 1\n",
    "            hasbreak = True\n",
    "            \n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update for ES and RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_updated_tdict = calculate_transition(EStrain_tup, transit_dict)\n",
    "RU_updated_tdict = calculate_transition(RUtrain_tup, transit_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert above calculation to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_param(updated_tdict):\n",
    "    all_states = ['O', 'B-positive', 'B-negative', 'B-neutral', 'I-positive', 'I-negative', 'I-neutral', 'START', 'STOP']\n",
    "    q_dict = {}\n",
    "    temp_sum = 0\n",
    "    for state in all_states:\n",
    "        total = sum(value for key, value in updated_tdict.items() if key[1] == state)\n",
    "        for key, value in updated_tdict.items():\n",
    "            if ( (key[1] == state) and ( total != 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = value/total\n",
    "            elif ( (key[1] == state) and ( total == 0) ):\n",
    "                q_dict[f\"{state}|{key[0]}\"] = 0\n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Below is the final parameters to be used*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Format is v|u (next state given current state)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_transit = transition_param(ES_updated_tdict)\n",
    "RU_transit = transition_param(RU_updated_tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting emission params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_emission_parameter, ES_emission_count, ES_label_count = train_estimate_emission_parameters(ES_train)\n",
    "ES_emission = upgraded_estimate_emission_parameters(ES_dev_in, ES_emission_parameter, ES_emission_count, ES_label_count)\n",
    "\n",
    "RU_emission_parameter, RU_emission_count, RU_label_count = train_estimate_emission_parameters(RU_train)\n",
    "RU_emission = upgraded_estimate_emission_parameters(RU_dev_in, RU_emission_parameter, RU_emission_count, RU_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dev.in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ES/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "ESdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "ESdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in ESdev ]\n",
    "\n",
    "with open('./RU/dev.in', 'r', encoding = 'utf8') as f:\n",
    "    file = f.readlines()\n",
    "RUdev = [lines.rstrip(\"\\n\") for lines in file]\n",
    "RUdev_ls = [ lines if lines != \"\" else \"STOP\" for lines in RUdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructuring dev.in to list of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ls(dev_in_ls, transit_param):\n",
    "    sequence_list = []\n",
    "    sequence_xi = []\n",
    "    for word in dev_in_ls:\n",
    "        if ( word != \"STOP\"):\n",
    "            sequence_xi.append(word)\n",
    "        else:\n",
    "            sequence_list.append(sequence_xi.copy())\n",
    "            sequence_xi = []\n",
    "    return sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_seq_list = to_ls(ESdev_ls, ES_transit)\n",
    "RU_seq_list = to_ls(RUdev_ls, RU_transit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on previous state, get the possible next states based on the current word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_states(prev_state, desired_word, emit, transit):\n",
    "    filtered_keys = [key for key in emit.keys() if (key[0] == desired_word)]\n",
    "    signal = False\n",
    "    if ( len(filtered_keys) == 0 ):\n",
    "        desired_pattern = f\"|{prev_state}\"\n",
    "        unk = [(key, value) for key, value in transit.items() if ( key.endswith(desired_pattern) and not key.startswith(\"STOP\") and not key.startswith(\"START\") and value!=0 )]\n",
    "        filtered_keys = [(desired_word, item[0].split('|')[0]) for item in unk]\n",
    "        signal = True\n",
    "    possible_states = [key[1] for key in filtered_keys]\n",
    "    return possible_states, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sequence, emit, transit, u, v, index, dt = {}):\n",
    "    u = v\n",
    "    dt[(0,v)] = 0\n",
    "    max_u_ls = []\n",
    "    possible_states = [\"START\"]\n",
    "    word = \"\"\n",
    "    for i in range(len(sequence)):\n",
    "        save_state = possible_states\n",
    "        word = sequence[index]\n",
    "        signal = False\n",
    "        possible_states, signal = get_possible_states(save_state[0], word, emit, transit)\n",
    "        for u in save_state:\n",
    "            for v in possible_states:\n",
    "                a = np.log(transit[f\"{v}|{u}\"])\n",
    "                if signal == True:\n",
    "                    b = np.log(emit[('#UNK#', v)])\n",
    "                else:\n",
    "                    b = np.log(emit[(word, v)])\n",
    "                max_u_ls.append(dt[(index, u)] + a + b)\n",
    "            for j in range(len(possible_states)):\n",
    "                key = ( index+1, possible_states[j])\n",
    "                if key not in dt:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "                elif dt[key] < max_u_ls[j]:\n",
    "                    dt[key] = max_u_ls[j]\n",
    "            max_u_ls = max_u_ls[len(possible_states):]\n",
    "        max_u_ls = []\n",
    "        index += 1\n",
    "    dct = dt.copy()\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(emit, transit, seq_list):\n",
    "    dt = {}\n",
    "    pred = []\n",
    "    for i in seq_list:\n",
    "        temp = viterbi(i, emit, transit, \"START\", \"START\", 0, {})\n",
    "        pred.append(temp)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Might have divide by zero error but it's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyo\\AppData\\Local\\Temp\\ipykernel_33580\\4078816607.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(transit[f\"{v}|{u}\"])\n"
     ]
    }
   ],
   "source": [
    "ES_pred = predict(ES_emission, ES_transit, ES_seq_list)\n",
    "RU_pred = predict(RU_emission, RU_transit, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each duplicated index, look for the one with highest log value a.k.a highest prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pred(pred):\n",
    "    pred_ls = []\n",
    "    grouped_dict = {}\n",
    "    for seq in pred:\n",
    "        for key, value in seq.items():\n",
    "            var = key[0]\n",
    "            if var in range(0, len(seq)):\n",
    "                if var not in grouped_dict or value > grouped_dict[var][1]:\n",
    "                    grouped_dict[var] = (key, value)\n",
    "\n",
    "        pred_ls.append([item[0] for item in grouped_dict.values()])\n",
    "        grouped_dict = {}\n",
    "    return pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_proc = processing_pred(ES_pred)\n",
    "RU_proc = processing_pred(RU_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(pred_res, seq_list):\n",
    "    res = pd.DataFrame(columns=['word', 'label'])\n",
    "    for seq_i in range(len(seq_list)):\n",
    "        labels = [label for _, label in pred_res[seq_i][1:]]\n",
    "        df = pd.DataFrame({\n",
    "            'word': seq_list[seq_i],\n",
    "            'label': labels\n",
    "        })\n",
    "        df.loc[len(df)] = ''\n",
    "        res = pd.concat([res, df])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2 = consolidate(ES_proc, ES_seq_list)\n",
    "RU_pred_q2 = consolidate(RU_proc, RU_seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_pred_q2.to_csv(\"./ES/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)\n",
    "RU_pred_q2.to_csv(\"./RU/dev.p2.out\", sep=\" \", header=None, index=False, encoding=\"utf-8\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring for q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES result is:\n",
      "\r\n",
      "#Entity in gold data: 229\r\n",
      "#Entity in prediction: 1014\r\n",
      "\r\n",
      "#Correct Entity : 139\r\n",
      "Entity  precision: 0.1371\r\n",
      "Entity  recall: 0.6070\r\n",
      "Entity  F: 0.2237\r\n",
      "\r\n",
      "#Correct Sentiment : 86\r\n",
      "Sentiment  precision: 0.0848\r\n",
      "Sentiment  recall: 0.3755\r\n",
      "Sentiment  F: 0.1384\r\n",
      "\n",
      "RU result is:\n",
      "\r\n",
      "#Entity in gold data: 389\r\n",
      "#Entity in prediction: 1392\r\n",
      "\r\n",
      "#Correct Entity : 234\r\n",
      "Entity  precision: 0.1681\r\n",
      "Entity  recall: 0.6015\r\n",
      "Entity  F: 0.2628\r\n",
      "\r\n",
      "#Correct Sentiment : 150\r\n",
      "Sentiment  precision: 0.1078\r\n",
      "Sentiment  recall: 0.3856\r\n",
      "Sentiment  F: 0.1684\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command2 = ['python', r'.\\ES\\evalResult.py', r'.\\ES\\dev.out', r'.\\ES\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"ES result is:\")\n",
    "print(result2.stdout.decode())\n",
    "\n",
    "command2 = ['python', r'.\\RU\\evalResult.py', r'.\\RU\\dev.out', r'.\\RU\\dev.p2.out']\n",
    "result2 = subprocess.run(command2, stdout=subprocess.PIPE)\n",
    "print(\"RU result is:\")\n",
    "print(result2.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
